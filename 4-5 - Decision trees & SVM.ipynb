{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee18427c",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "In this unit, we will illustrate **decision trees** using the Titanic data. Our goal is to build a model that can predict whether a passenger from the Titanic survived or not based on several features including its age, sex and passenger class (1st, 2nd or 3rd).\n",
    "\n",
    "In the first part of this unit, we will see the basic idea behind decision trees. We will then build one for the Titanic data set using Scikit-learn. You can download the data set from the resource section.\n",
    "\n",
    "The idea behind decision trees is to learn a set of **if-then-else rules** that lead to a final decision. In classification tasks, this decision is a label. This may sound abstract, so let's take an example. Here is a simple decision tree with two if-then-else statements that encode the principle \"women and children first\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b17862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2812/decision_trees.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2812/decision_trees.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80489ada",
   "metadata": {},
   "source": [
    "In this image, the squares in gray are binary statements that can be either true or false. Each outcome corresponds to a branch of the three and leads to another statement (called a **node**) or to a final label (called a **leaf**). In the Titanic example, the age and sex of the passengers are features, and we want to build a tree that leads to one of the two labels in our classification task: survived or died. To classify a new sample, we start at the top of the tree (called its **root**) and evaluate each statement on the path until reaching a leaf.\n",
    "\n",
    "### Impurity measures\n",
    "\n",
    "The main challenge here is to create meaningful rules to separate our data set into \"pure\" subsets that have samples with similar target values. In our binary classification case, we want the leaves to have most of the samples labeled with either \"died\" or \"survived\". We can quantify how \"pure\" each leaf is by counting the proportion of entries labeled with the positive class (an estimation of $p(y=1)$ at the leaf). If this probability is close to 0 or 1, then it means that the leaf is \"pure\" in the sense that most of its samples are from the same class.\n",
    "\n",
    "In practice, we want to build rules that minimize an **impurity measure** at each leaf. For instance, we can measure the **misclassification impurity** which is simply the error rate at a leaf when predicting the most frequent class.\n",
    "\n",
    "> $Imisclassification(p)=1−max(p,1−p)$\n",
    "\n",
    "For instance, if 80% of the samples at a leaf are labeled with \"survived\", then \n",
    "p is 0.8, and the misclassification impurity is 20% since we expect to have an error rate of 20% when predicting the label \"survived\" at that leaf. Note that there are other possible impurity measures. For instance, we can use **cross-entropy (CE)** which is a way to quantify uncertainty (or disorder).\n",
    "\n",
    "> $Icross-entropy(p)=−p.log2(p)−(1−p).log2(1−p)$\n",
    "\n",
    "However, in practice, we prefer to use **Gini impurity** which is very similar to cross-entropy but faster to compute.\n",
    "\n",
    "> $Igini(p)=2p(1−p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79b9647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/5638/impurity-functions.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/5638/impurity-functions.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632dc8b",
   "metadata": {},
   "source": [
    "Note that the misclassification and Gini measures range between 0 and 0.5 while cross-entropy between 0 and 1. In this image, we rescaled cross-entropy such that all impurity functions have a maximum value of 0.5.\n",
    "\n",
    "As we can see, all functions have an impurity of zero when the set contains only examples from one class (\n",
    "p\n",
    "=\n",
    "0\n",
    " or \n",
    "p\n",
    "=\n",
    "1\n",
    "), and the impurity is maximal when half of the samples come from each class (\n",
    "p\n",
    "=\n",
    "0.5\n",
    ").\n",
    "\n",
    "### Building the decision tree\n",
    "\n",
    "When building a decision tree, we start at the top of the tree and iteratively try to find a split that minimizes the impurity at the leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd199b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2810/split_impurity.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2810/split_impurity.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e75dcc",
   "metadata": {},
   "source": [
    "In this image, the box in gray corresponds to a condition on a feature (e.g., age lower than 12, or passenger class is 1st). We denote the impurity measure at each leaf with \n",
    "$I(pL)$ and $I(pR)$ where $pL$ and $pR$ are the probabilities of the positive class in the left and right leaves. Our goal is to find a split that minimizes the **total impurity**.\n",
    "\n",
    "> $Isplit=nLI(pL)+nRI(pR)$\n",
    "\n",
    "In this equation, we compute the split impurity $Isplit$ by summing the impurity values at each leaf weighted by their number of samples $nL$ and $nR$. In a typical scenario, we test different conditions for each feature in our data set and **select the one with the minimum split impurity**. We can then continue to split the leaves until we get \"pure\" leaves with samples from a single class. In practice, the rules become more specific after each split, and the tree starts overfitting. For this reason, we usually set a limit on its depth.\n",
    "\n",
    "We will now go through a concrete example and build a decision tree for the Titanic data set using Scikit-learn.\n",
    "\n",
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508ee696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hirvonen, Miss Hildur E</td>\n",
       "      <td>3rd</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Van Impe, Miss Catharine</td>\n",
       "      <td>3rd</td>\n",
       "      <td>10</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sjostedt, Mr Ernst Adolf</td>\n",
       "      <td>2nd</td>\n",
       "      <td>59</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fortune, Mr Mark</td>\n",
       "      <td>1st</td>\n",
       "      <td>64</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ware, Mr William J</td>\n",
       "      <td>2nd</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name pclass  age     sex  survived\n",
       "0   Hirvonen, Miss Hildur E    3rd    2  female         0\n",
       "1  Van Impe, Miss Catharine    3rd   10  female         0\n",
       "2  Sjostedt, Mr Ernst Adolf    2nd   59    male         0\n",
       "3          Fortune, Mr Mark    1st   64    male         0\n",
       "4        Ware, Mr William J    2nd   23    male         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# First five rows\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ad597",
   "metadata": {},
   "source": [
    "The data contains five variables. The features are the name of the passenger, its class, age and sex, and the target variable indicates if the passenger survived.\n",
    "\n",
    "Scikit-learn cannot work directly on this DataFrame to build the decision tree, but we can convert it into the usual X/y Numpy arrays by encoding categories with numerical values. We will use a simple 0/1 encoding for the binary sex feature and one-hot encoding for the pclass variable which has three possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed51525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass_1st</th>\n",
       "      <th>pclass_2nd</th>\n",
       "      <th>pclass_3rd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hirvonen, Miss Hildur E</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Van Impe, Miss Catharine</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sjostedt, Mr Ernst Adolf</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fortune, Mr Mark</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ware, Mr William J</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  age  sex  survived  pclass_1st  pclass_2nd  \\\n",
       "0   Hirvonen, Miss Hildur E    2    1         0           0           0   \n",
       "1  Van Impe, Miss Catharine   10    1         0           0           0   \n",
       "2  Sjostedt, Mr Ernst Adolf   59    0         0           0           1   \n",
       "3          Fortune, Mr Mark   64    0         0           1           0   \n",
       "4        Ware, Mr William J   23    0         0           0           1   \n",
       "\n",
       "   pclass_3rd  \n",
       "0           1  \n",
       "1           1  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "encoded_df = pd.get_dummies(data_df, columns=['pclass'])\n",
    "\n",
    "# Encode binary variables with 0s and 1s\n",
    "encoded_df['sex'] = encoded_df.sex.replace({\n",
    "    'male': 0,\n",
    "    'female': 1\n",
    "})\n",
    "\n",
    "# First five rows\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd20ce6",
   "metadata": {},
   "source": [
    "In this code, we create a dummy variable for each passenger class using the get_dummies() function from Pandas, and we replace the male and female values with numerical 0/1 labels. The encoded_df DataFrame now only contains numerical values (except the name variable, but we will drop it before building the tree).\n",
    "\n",
    "We can now create the X/y arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580c7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Select features\n",
    "features = encoded_df.drop(['name', 'survived'], axis=1)\n",
    "\n",
    "# Create X/y arrays\n",
    "X = features.values\n",
    "y = encoded_df.survived.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ca90a",
   "metadata": {},
   "source": [
    "At the moment, Scikit-learn doesn't support categorical variables. Hence, we need to encode them using one-hot encoding. However, note that there is an ongoing effort by the community to solve this. You can take a look at this link if you want to learn more about it.\n",
    "\n",
    "https://github.com/scikit-learn/scikit-learn/pull/4899\n",
    "\n",
    "### Scikit-learn DecisionTreeClassifier\n",
    "\n",
    "Scikit-learn provides a DecisionTreeClassifier estimator to build decision trees. Let's start with a simple decision tree with a depth of one that minimizes the Gini impurity measure at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95d6559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create decision tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    criterion='gini', max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01fe5a",
   "metadata": {},
   "source": [
    "The estimator shuffles the features at each split. Hence, the results may vary when there are ties (two splits with the same impurity score). In the code from above, we set the random_state parameter to zero to fix the results.\n",
    "\n",
    "We can now create and evaluate the decision tree using the fit() and the score() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12241800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7791005291005291"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit decision tree\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Get score\n",
    "dt.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd646b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.585979\n",
       "1    0.414021\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of features in each class\n",
    "pd.value_counts(y, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba1bcd",
   "metadata": {},
   "source": [
    "We get an accuracy around 78% which is already much better than the \"most-frequent\" baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98157518",
   "metadata": {},
   "source": [
    "Note that we removed part of the entries labeled with \"died\" in the titanic.csv file to have a more balanced the data set (the survival rate is around 30% according to Wikipedia and not 40% as above). **Decision trees try to minimize the impurity at each split**. Hence, if there are more samples from one class than the others, **it will focus on building rules that correctly classify data points from this class**. In our case, we want to build a decision tree that can classify well samples from both categories. **Hence, it's important that they have approximately the same number of samples.**\n",
    "\n",
    "**This problem is not specific to decision tree**s. For instance, a k-NN classifier will also favor points from the dominant classes when doing the majority vote. This is called the **class imbalance problem.**\n",
    "\n",
    "> Note: Another way to solve the issue is to modify the objective function of our classifiers and put more weight on classes that are underrepresented in the training set. We won't go into the details in this course, but note that Scikit-learn implements this strategy with the class_weight parameter that you can set to balanced when it's important that your classifier performs well with all classes independently of how many samples you have from each in your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57f835",
   "metadata": {},
   "source": [
    "### Visualize decision trees\n",
    "\n",
    "One of the main advantages of decision trees is that they are easy to interpret.\n",
    "\n",
    "Each node corresponds to a simple rule.\n",
    "The most important ones are at the top of the decision tree.\n",
    "We can visualize them using a library called **Graphvi**. Scikit-learn implements an export_graphviz() function to export decision tree estimators into the Graphviz .dot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Export decision tree\n",
    "dot_data = export_graphviz(\n",
    "    dt, out_file=None,\n",
    "    feature_names=features.columns, class_names=['died', 'survived'],\n",
    "    filled=True, rounded=True, proportion=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e2c75",
   "metadata": {},
   "source": [
    "In this code, we create a dot_data variable from our decision tree dt. This variable contains all the information to plot our decision tree. Note that the function provides many arguments to customize its appearance and to specify what information it contains. In our case, there are three important arguments.\n",
    "\n",
    "* feature_names - the names of the features in X\n",
    "* class_names - the names of the classes in y\n",
    "* proportion=True - to include the proportion of samples in each node\n",
    "\n",
    "Note that we set the feature names using the features variable from above (encoded_df without its name and survived columns). We won't go into the details of each argument, but you can take a look at this page if you want to learn more about them.\n",
    "\n",
    "To visualize the decision tree, we need to load it into a Source object from the Graphviz library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a613299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"334pt\" height=\"195pt\"\r\n",
       " viewBox=\"0.00 0.00 334.00 195.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 191)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-191 330,-191 330,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.294118\" stroke=\"black\" d=\"M228,-187C228,-187 98,-187 98,-187 92,-187 86,-181 86,-175 86,-175 86,-116 86,-116 86,-110 92,-104 98,-104 98,-104 228,-104 228,-104 234,-104 240,-110 240,-116 240,-116 240,-175 240,-175 240,-181 234,-187 228,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sex &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.485</text>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100.0%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.586, 0.414]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = died</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.741176\" stroke=\"black\" d=\"M142,-68C142,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 142,-0 142,-0 148,-0 154,-6 154,-12 154,-12 154,-56 154,-56 154,-62 148,-68 142,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.326</text>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 61.9%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.795, 0.205]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = died</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.977,-103.726C123.957,-94.7878 116.518,-85.3168 109.48,-76.3558\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"112.082,-74.0022 103.153,-68.2996 106.577,-78.3259 112.082,-74.0022\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"100.169\" y=\"-89.4209\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.674510\" stroke=\"black\" d=\"M314,-68C314,-68 184,-68 184,-68 178,-68 172,-62 172,-56 172,-56 172,-12 172,-12 172,-6 178,-0 184,-0 184,-0 314,-0 314,-0 320,-0 326,-6 326,-12 326,-12 326,-56 326,-56 326,-62 320,-68 314,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.372</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 38.1%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.247, 0.753]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = survived</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195.023,-103.726C202.043,-94.7878 209.482,-85.3168 216.52,-76.3558\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.423,-78.3259 222.847,-68.2996 213.918,-74.0022 219.423,-78.3259\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"225.831\" y=\"-89.4209\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1343f4334e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "# Display decision tree\n",
    "graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6dcf2",
   "metadata": {},
   "source": [
    "The graph encodes classes using colors. Nodes with a majority of samples labeled with \"survived\" appear in blue and nodes with a majority of samples labeled with \"died\" in orange. Each node contains the following information.\n",
    "\n",
    "* gini - the impurity measure at this node\n",
    "* samples - the percentage of samples in this node\n",
    "* value - the probabilities for each class\n",
    "* class - the class with the highest probability\n",
    "\n",
    "The decision tree splits the data into two groups using a single rule on the sex of the passengers. **This feature is the most important one according to the Gini impurity measure.** Note that the node splits the samples by setting a threshold on the numerical label of the sex feature. Entries encoded with a zero (male) are in the left leaf and entries encoded with a one (female) are in the right leaf.\n",
    "\n",
    "According to the samples entries in the image from above, there are respectively 61.9% and 38.1% of the samples in the left and right leaves. We can verify that using the value_counts() function from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e5d0166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      0.619048\n",
       "female    0.380952\n",
       "Name: sex, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of samples by sex\n",
    "pd.value_counts(data_df.sex, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6ca20",
   "metadata": {},
   "source": [
    "The value entries in each leaf indicate the proportion of samples in each class. For instance, the left leaf has 79.5% of samples labeled with \"died\" and the remaining 20.5% labeled with \"survived\". Again, we can verify that using the crosstab() function from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee8b2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>survived</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>0.246528</td>\n",
       "      <td>0.753472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.205128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "survived         0         1\n",
       "sex                         \n",
       "female    0.246528  0.753472\n",
       "male      0.794872  0.205128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross tabulation of sex and survived\n",
    "crosstab = pd.crosstab(\n",
    "    index=data_df.sex,\n",
    "    columns=data_df.survived,\n",
    "    normalize='index' # Normalize by sex\n",
    ")\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b1085",
   "metadata": {},
   "source": [
    "The first row corresponds to the probabilities in the right leaf and the second row to the probabilities in the left one. Finally, let's use these probabilities to verify the Gini impurity values at each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a35bcbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini impurity (left leaf): 0.326\n",
      "Gini impurity (right leaf): 0.372\n"
     ]
    }
   ],
   "source": [
    "# Get p(survived) at each leaf\n",
    "p_left = crosstab.loc['male', 1]\n",
    "p_right = crosstab.loc['female', 1]\n",
    "\n",
    "# Compute Gini impurity values\n",
    "i_left = 2*p_left*(1-p_left)\n",
    "i_right = 2*p_right*(1-p_right)\n",
    "print('Gini impurity (left leaf): {:.3f}'.format(i_left))\n",
    "# Prints: 0.326\n",
    "print('Gini impurity (right leaf): {:.3f}'.format(i_right))\n",
    "# Prints: 0.372"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7263b",
   "metadata": {},
   "source": [
    "### Deeper trees\n",
    "Let's try to increase the depth of the tree to two with the max_depth parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "724b9144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8108465608465608"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create decision tree\n",
    "dt_2 = DecisionTreeClassifier(\n",
    "    criterion='gini', max_depth=2, random_state=0)\n",
    "\n",
    "# Fit decision tree\n",
    "dt_2.fit(X, y)\n",
    "\n",
    "# Get score\n",
    "dt_2.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a7125d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"678pt\" height=\"314pt\"\r\n",
       " viewBox=\"0.00 0.00 678.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-310 674,-310 674,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.294118\" stroke=\"black\" d=\"M400,-306C400,-306 270,-306 270,-306 264,-306 258,-300 258,-294 258,-294 258,-235 258,-235 258,-229 264,-223 270,-223 270,-223 400,-223 400,-223 406,-223 412,-229 412,-235 412,-235 412,-294 412,-294 412,-300 406,-306 400,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"335\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sex &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"335\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.485</text>\r\n",
       "<text text-anchor=\"middle\" x=\"335\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100.0%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"335\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.586, 0.414]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"335\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = died</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.741176\" stroke=\"black\" d=\"M314,-187C314,-187 184,-187 184,-187 178,-187 172,-181 172,-175 172,-175 172,-116 172,-116 172,-110 178,-104 184,-104 184,-104 314,-104 314,-104 320,-104 326,-110 326,-116 326,-116 326,-175 326,-175 326,-181 320,-187 314,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">age &lt;= 12.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.326</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 61.9%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.795, 0.205]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = died</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M305.163,-222.907C298.626,-214.014 291.639,-204.509 284.893,-195.331\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287.527,-193.005 278.785,-187.021 281.887,-197.151 287.527,-193.005\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"275.011\" y=\"-208.034\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.674510\" stroke=\"black\" d=\"M486,-187C486,-187 356,-187 356,-187 350,-187 344,-181 344,-175 344,-175 344,-116 344,-116 344,-110 350,-104 356,-104 356,-104 486,-104 486,-104 492,-104 498,-110 498,-116 498,-116 498,-175 498,-175 498,-181 492,-187 486,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">pclass_3rd &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.372</text>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 38.1%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.247, 0.753]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = survived</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>0&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M364.837,-222.907C371.374,-214.014 378.361,-204.509 385.107,-195.331\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"388.113,-197.151 391.215,-187.021 382.473,-193.005 388.113,-197.151\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"394.989\" y=\"-208.034\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.635294\" stroke=\"black\" d=\"M142,-68C142,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 142,-0 142,-0 148,-0 154,-6 154,-12 154,-12 154,-56 154,-56 154,-62 148,-68 142,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.391</text>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4.0%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.267, 0.733]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = survived</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M184.954,-103.726C169.544,-93.9161 153.127,-83.4644 137.867,-73.7496\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"139.348,-70.5431 129.032,-68.1252 135.588,-76.4481 139.348,-70.5431\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.796078\" stroke=\"black\" d=\"M314,-68C314,-68 184,-68 184,-68 178,-68 172,-62 172,-56 172,-56 172,-12 172,-12 172,-6 178,-0 184,-0 184,-0 314,-0 314,-0 320,-0 326,-6 326,-12 326,-12 326,-56 326,-56 326,-62 320,-68 314,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.281</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 57.9%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.831, 0.169]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"249\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = died</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M249,-103.726C249,-95.5175 249,-86.8595 249,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"252.5,-78.2996 249,-68.2996 245.5,-78.2996 252.5,-78.2996\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.913725\" stroke=\"black\" d=\"M486,-68C486,-68 356,-68 356,-68 350,-68 344,-62 344,-56 344,-56 344,-12 344,-12 344,-6 350,-0 356,-0 356,-0 486,-0 486,-0 492,-0 498,-6 498,-12 498,-12 498,-56 498,-56 498,-62 492,-68 486,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.148</text>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 24.6%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.081, 0.919]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"421\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = survived</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421,-103.726C421,-95.5175 421,-86.8595 421,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"424.5,-78.2996 421,-68.2996 417.5,-78.2996 424.5,-78.2996\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.180392\" stroke=\"black\" d=\"M658,-68C658,-68 528,-68 528,-68 522,-68 516,-62 516,-56 516,-56 516,-12 516,-12 516,-6 522,-0 528,-0 528,-0 658,-0 658,-0 664,-0 670,-6 670,-12 670,-12 670,-56 670,-56 670,-62 664,-68 658,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"593\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.495</text>\r\n",
       "<text text-anchor=\"middle\" x=\"593\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13.5%</text>\r\n",
       "<text text-anchor=\"middle\" x=\"593\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0.549, 0.451]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"593\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = died</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>4&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.046,-103.726C500.456,-93.9161 516.873,-83.4644 532.133,-73.7496\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"534.412,-76.4481 540.968,-68.1252 530.652,-70.5431 534.412,-76.4481\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1343f562908>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export decision tree\n",
    "dot_data_2 = export_graphviz(\n",
    "    dt_2, out_file=None,\n",
    "    feature_names=features.columns, class_names=['died', 'survived'],\n",
    "    filled=True, rounded=True, proportion=True   \n",
    ")\n",
    "\n",
    "# Display it\n",
    "graphviz.Source(dot_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fd42d",
   "metadata": {},
   "source": [
    "Again, one of the main advantages of the decision tree model is **its interpretability**. Here is a textual summary of the tree.\n",
    "\n",
    "* Most men perished but young boys survived\n",
    "* Most women survived, but half of the women from the 3rd class perished\n",
    "\n",
    "However, the complexity of the tree increases with its depth, and it's easy to create a tree that overfits. For instance, let's remove the maximum depth criteria by setting it to None. This time, we will fit and evaluate the decision tree on different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86298276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.870\n",
      "Test accuracy: 0.784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train/test sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create decision tree\n",
    "dt_max_depth = DecisionTreeClassifier(\n",
    "    criterion='gini', max_depth=None, random_state=0)\n",
    "\n",
    "# Fit decision tree\n",
    "dt_max_depth.fit(X_tr, y_tr)\n",
    "\n",
    "# Get train/test scores\n",
    "print('Train accuracy: {:.3f}'.format(\n",
    "    dt_max_depth.score(X_tr, y_tr)))\n",
    "# Prints: 0.870\n",
    "\n",
    "print('Test accuracy: {:.3f}'.format(\n",
    "    dt_max_depth.score(X_te, y_te)))\n",
    "# Prints: 0.784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4844b1",
   "metadata": {},
   "source": [
    "As we can see, the decision tree correctly classifies 87% of the samples from the train set, but only 78.4% from the test set. **This is one of the main issues with decision trees. Small trees can be too simple (high bias) to model our data, and larger ones tend to overfit (high variance).**\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this unit, we saw the basic idea behind decision trees and implemented one using Scikit-learn. Here are a few takeaways.\n",
    "\n",
    "* Decision trees minimize an impurity measure at each split.\n",
    "* The complexity of a decision tree depends on its depth.\n",
    "* Scikit-learn DecisionTreeClassifier estimator requires categorical features to be encoded with one-hot encoding.\n",
    "\n",
    "In practice, we use a decision tree when we want to build a **simple model that we can easily visualize and interpret**. However, we will see in the next unit that we can also combine **large trees into a random forest to create more complex models.**\n",
    "\n",
    "### Question about multi class GINI\n",
    "https://scikit-learn.org/stable/modules/tree.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7577c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
