{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb02b0a",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb58b8",
   "metadata": {},
   "source": [
    "So far, we used Scikit-learn for its **ML estimators like the LinearRegression or Ridge ones and saw a few examples of transformers with the StandardScaler and PolynomialFeatures objects.** In this unit and the next ones, we will see **how to assemble Scikit-learn estimators and transformers into a full ML pipeline.**\n",
    "\n",
    "In this unit, we will see how to use the Pipeline object from Scikit-learn using the bike sharing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3493e0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>yr</th>\n",
       "      <th>workingday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.249</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.248</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.227</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.187</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed    yr workingday holiday  weekday  season  \\\n",
       "0  0.344  0.806      0.160  2011         no      no        6  spring   \n",
       "1  0.363  0.696      0.249  2011         no      no        0  spring   \n",
       "2  0.196  0.437      0.248  2011        yes      no        1  spring   \n",
       "3  0.200  0.590      0.160  2011        yes      no        2  spring   \n",
       "4  0.227  0.437      0.187  2011        yes      no        3  spring   \n",
       "\n",
       "  weathersit  casual  \n",
       "0     cloudy     331  \n",
       "1     cloudy     131  \n",
       "2      clear     120  \n",
       "3      clear     108  \n",
       "4      clear      82  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('bike-sharing.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7143dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# One-hot encoding\n",
    "encoded_df = pd.get_dummies(data_df)\n",
    "\n",
    "# Split into train/test sets\n",
    "X = encoded_df.drop('casual', axis=1).values\n",
    "y = data_df.casual.values\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c06a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_tr_rescaled = scaler.fit_transform(X_tr)\n",
    "X_te_rescaled = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d190e155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_tr_rescaled, y_tr)\n",
    "print('MAE: {:.0f}'.format(MAE(y_te, ridge.predict(X_te_rescaled))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff62bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median baseline: 523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "median_predictions = np.full_like(y_te, np.median(y_tr))\n",
    "print('Median baseline: {:.0f}'.format(MAE(y_te, median_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e4ac0",
   "metadata": {},
   "source": [
    "Applying different preprocessing steps to the train/test data is a common mistake which **can be hard to detect when we have a long sequence of transformations**. To solve this issue, we can **use the Pipeline object to encapsulate them with the estimator.** Let's see how to use this object.\n",
    "\n",
    "### Pipeline Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6795a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26445ec5",
   "metadata": {},
   "source": [
    "The steps are simply (name, object) pairs. For each object, we can choose a name that we can then use later to refer to the step. In this case, we simply name the StandardScaler() step scaler and the Ridge() one ridge.\n",
    "\n",
    "We can then retrieve the list of steps with **the named_steps** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1ee3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'ridge': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "    normalize=False, random_state=None, solver='auto', tol=0.001)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a dictionary with each step\n",
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac6fc0",
   "metadata": {},
   "source": [
    "**The pipeline object can be used as a standard Scikit-learn estimator i.e. it implements the usual fit(), predict() and score() functions from the estimator API.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3216fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 280\n"
     ]
    }
   ],
   "source": [
    "# Fit on the train set\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print('MAE: {:.0f}'.format(MAE(y_te, pipe.predict(X_te))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae826aef",
   "metadata": {},
   "source": [
    "### Multiple preprocessing steps\n",
    "\n",
    "In the example above, we have a single preprocessing step. The pipe.fit() call is equivalent to applying the following handmade function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c029ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_fit(X, y):\n",
    "    # Fit and apply the transformation\n",
    "    X1 = scaler.fit_transform(X)\n",
    "\n",
    "    # Fit the estimator\n",
    "    ridge.fit(X1, y)\n",
    "\n",
    "# Fit to the train data\n",
    "pipe_fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb437ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 280\n"
     ]
    }
   ],
   "source": [
    "def pipe_predict(X):\n",
    "    # Apply the transformation\n",
    "    X1 = scaler.transform(X)\n",
    "\n",
    "    # Make predictions\n",
    "    return ridge.predict(X1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print('MAE: {:.0f}'.format(MAE(y_te, pipe_predict(X_te))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20b8ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with three transformations\n",
    "# pipe = Pipeline([\n",
    "#     ('transform1', ...),\n",
    "#     ('transform2', ...),\n",
    "#     ('transform3', ...),\n",
    "#     ('estimator', ...)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7afc2663",
   "metadata": {},
   "source": [
    "function to pipe.fit()\n",
    "\n",
    "# Example with three transformations\n",
    "def pipe_fit(X, y):\n",
    "    # Fit and apply the transformations\n",
    "    X1 = transform1.fit_transform(X)\n",
    "    X2 = transform2.fit_transform(X1)\n",
    "    X3 = transform3.fit_transform(X2)\n",
    "\n",
    "    # Fit the estimator\n",
    "    estimator.fit(X3, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f98218",
   "metadata": {},
   "source": [
    "### Optional steps\n",
    "It can be interesting to try fitting an estimator with and without a preprocessing step to see if it improves to results. We will see a few scenarios in the next course where this is helpful. To achieve this, we can simply set the step to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708854c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 282\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', None), # Disable this step\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Fit pipeline to the train set\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('MAE: {:.0f}'.format(MAE(y_te, pipe.predict(X_te))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af6c8a",
   "metadata": {},
   "source": [
    "### Grid search with pipelines\n",
    "Let's see how we would tune the alpha hyperparameter using grid search on our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d601ef07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>train_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>279.754483</td>\n",
       "      <td>262.057394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>279.754483</td>\n",
       "      <td>262.057392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000145</td>\n",
       "      <td>279.754485</td>\n",
       "      <td>262.057391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000175</td>\n",
       "      <td>279.754486</td>\n",
       "      <td>262.057389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000210</td>\n",
       "      <td>279.754487</td>\n",
       "      <td>262.057386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha    test_mae   train_mae\n",
       "0  0.000100  279.754483  262.057394\n",
       "1  0.000120  279.754483  262.057392\n",
       "2  0.000145  279.754485  262.057391\n",
       "3  0.000175  279.754486  262.057389\n",
       "4  0.000210  279.754487  262.057386"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Variable to store the results\n",
    "gs_results = []\n",
    "\n",
    "# Grid search\n",
    "for alpha in np.logspace(-4, 4, num=100):\n",
    "    # Create/fit the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha))\n",
    "    ])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    # Save model and its performance on train/test sets\n",
    "    gs_results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_mae': MAE(y_tr, pipe.predict(X_tr)),\n",
    "        'test_mae': MAE(y_te, pipe.predict(X_te)),\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "gs_results = pd.DataFrame(gs_results)\n",
    "gs_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab875c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the validation curves\n",
    "plt.semilogx(gs_results['alpha'], gs_results['train_mae'], label='train curve')\n",
    "plt.semilogx(gs_results['alpha'], gs_results['test_mae'], label='test curve')\n",
    "plt.xlabel('$alpha$')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571082d",
   "metadata": {},
   "source": [
    "The model is not overfitting: the test curve remains flat when releasing the regularization constraint i.e. alpha values close to zero. On the other hand, we can see that the performance quickly drops when increasing alpha. This corresponds to the underfitting case.\n",
    "\n",
    "If we don't want to create a new pipeline object in the for loop (ex. for readability), we can also use the **get_params() and set_params() functions** which are available in every Scikit-learn estimator including the Pipeline one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1837aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('scaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('ridge',\n",
       "   Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001))],\n",
       " 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'ridge': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "    normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       " 'scaler__copy': True,\n",
       " 'scaler__with_mean': True,\n",
       " 'scaler__with_std': True,\n",
       " 'ridge__alpha': 1.0,\n",
       " 'ridge__copy_X': True,\n",
       " 'ridge__fit_intercept': True,\n",
       " 'ridge__max_iter': None,\n",
       " 'ridge__normalize': False,\n",
       " 'ridge__random_state': None,\n",
       " 'ridge__solver': 'auto',\n",
       " 'ridge__tol': 0.001}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb2e5e",
   "metadata": {},
   "source": [
    "In this case, the dictionary of parameters from our pipe estimator contains all the steps with their hyperparameters prefixed by the name of the step and two underscores _. For instance, ridge__alpha corresponds to the regularization strength of the ridge step.\n",
    "\n",
    "We can use the same syntax to set each hyperparameter value. For instance, let's set the alpha one in our grid-search loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b7237e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-06075023b2f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Fit the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mridge__alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Variable to store the results\n",
    "gs_results = []\n",
    "\n",
    "# Grid search\n",
    "for alpha in np.logspace(-4, 4, num=100):\n",
    "    # Fit the pipeline\n",
    "    pipe.set_params(ridge__alpha=alpha)\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    # Save model and its performance on train/test sets\n",
    "    gs_results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_mae': MAE(y_tr, pipe.predict(X_tr)),\n",
    "        'test_mae': MAE(y_te, pipe.predict(X_te)),\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "gs_results = pd.DataFrame(gs_results)\n",
    "\n",
    "# Plot the validation curves\n",
    "plt.semilogx(gs_results['alpha'], gs_results['train_mae'], label='train curve')\n",
    "plt.semilogx(gs_results['alpha'], gs_results['test_mae'], label='test curve')\n",
    "plt.xlabel('$alpha$')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e1033",
   "metadata": {},
   "source": [
    "Both approaches are correct and produce the same results. However, the set_params() one can be useful to quickly test different hyperparameter values without having to define new pipeline objects.\n",
    "\n",
    "### Summary\n",
    "In this unit, we saw how to encapsulate a set of preprocessing steps and an estimator into a single Pipeline object. It's interesting to note that we included only the standardization step in the pipeline and not the one-hot encoding one which was performed outside Scikit-learn with the get_dummies() function from Pandas.\n",
    "\n",
    "In the next unit, **we will see how to encode categorical variables directly in Scikit-learn.** To achieve this, **we will use the ColumnTransformer object which is part of a new workflow that tries to consolidate the data manipulation steps**, usually done in Pandas, with the modeling part from Scikit-learn.\n",
    "\n",
    "# Column transformations\n",
    "\n",
    "Perform the data preparation, exploratory data analysis (EDA), feature encoding/engineering, outliers removal in Pandas before the ML modeling as we have seen so far in this course. When the data is in a good format and ready for the ML models, convert the DataFrame to a Numpy 2d float array with .values and pass it to the sklearn estimator .fit/predict/score methods or Pipeline object if a StandardScaler is needed.\n",
    "\n",
    "**Explanation:** So far in this program, we have seen how to structure our data analysis into (1) data preparation (2) exploratory data analysis (EDA) and (3) machine learning parts. The first two steps (1) and (2) involve a lot of data manipulation and are done in Pandas and the last one (3) with Scikit-learn. Because ML models only work with numerical data, we usually convert our DataFrame into a Numpy float 2d array only at step (3) for sklearn estimators. It's important to understand that the data preprocessing (1) and (2) are done in Pandas. The only exception is for common preprocessing steps that are very specific to ML such as StandardScaler or dimensionality reduction such as PCA (more about this in the next course) - those are usually encapsulated into a Pipeline object as shown in the last unit. The reason for this exception is simple: those ML operations are independent of the nature of the column unlike the data manipulation steps from (1) and (2) that are usually very specific to each variable. For instance, feature engineering and outliers removal are done in Pandas because they depend on the type of variable and their meaning ex. it doesn't make sense to create polynomial features for categories or apply z-score outliers removal to ordinal variables or skewed ones.\n",
    "\n",
    "**In this unit and the next ones:** Jupyter notebooks are a great way to develop/share a data analysis pipeline and document each step with Markdown cells and plots in an iterative way. At the end of this \"prototyping\" work, we sometimes want to encapsulate our code from (1) and (2) into a **\"clean\" ML pipeline.** In this unit and the next ones from this Advanced Scikit-learn chapter, we will see tools to achieve this. However, note that it's not required to use those tools - they are only helpful to do this **extra step** of encapsulating the Pandas code from steps (1) and (2) into Scikit-learn objects **at the end of the analysis/prototyping work.** For this reason, this unit and the next ones from this chapter are entirely optional. You can skip those units and start working now on the final course project. When you are happy with your work, you can optionally read this unit and the next ones and think about how you could apply those tools to your analysis. However, this is entirely optional and requires good programming/debugging experience.\n",
    "\n",
    "\n",
    "# PAS OBLIGATOIRE ==> SCIKIT LEARN PREPROCESSING\n",
    "### Column transformations\n",
    "In this unit, we will see how to use the ColumnTransformer object from Scikit-learn to perform a few common preprocessing steps such as ordinal and one-hot encoding.\n",
    "\n",
    "Before going into the code, it's important to understand that this tool is part of a new workflow in Scikit-learn that tries to consolidate the data manipulation steps, usually done in Pandas, with the modeling part. As we will see in this unit and the next ones, this new Pandas/Scikit-learn workflow can be very powerful - however - Scikit-learn only provides partial support for DataFrames at the moment, so **it can be difficult to model complex sequences of data manipulations with it.**\n",
    "\n",
    "In such cases, don't hesitate to do part or all of the data manipulation work in Pandas as we saw previously. Also, keep an eye on the upcoming Scikit-learn releases to see how these new features evolve.\n",
    "\n",
    "### One-hot encoding with Scikit-learn\n",
    "Let's start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c77089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>yr</th>\n",
       "      <th>workingday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.249</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.248</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.227</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.187</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed    yr workingday holiday  weekday  season  \\\n",
       "0  0.344  0.806      0.160  2011         no      no        6  spring   \n",
       "1  0.363  0.696      0.249  2011         no      no        0  spring   \n",
       "2  0.196  0.437      0.248  2011        yes      no        1  spring   \n",
       "3  0.200  0.590      0.160  2011        yes      no        2  spring   \n",
       "4  0.227  0.437      0.187  2011        yes      no        3  spring   \n",
       "\n",
       "  weathersit  casual  \n",
       "0     cloudy     331  \n",
       "1     cloudy     131  \n",
       "2      clear     120  \n",
       "3      clear     108  \n",
       "4      clear      82  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('bike-sharing.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed4fe4",
   "metadata": {},
   "source": [
    "Scikit-learn implements a OneHotEncoder transformer to handle categorical variables. Like the other objects from Scikit-learn, it accepts array-like objects, including DataFrames, as input but always returns Numpy arrays or related objects as we are will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223066c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<731x1714 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7310 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create encoder\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit_transform(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00d050",
   "metadata": {},
   "source": [
    "The result can be a bit surprising at first sight: we pass a DataFrame object and get a sparse matrix with 1,714 columns! In fact, **the transformer encodes all the columns from the input data, including the numerical ones.** So it creates a new one-hot encoded column for each distinct value in the DataFrame.\n",
    "\n",
    "**Let's see how to fix this.**\n",
    "\n",
    "### ColumnTransformer object\n",
    "\n",
    "So far, we always converted the input data into Numpy arrays to avoid any issues during the ml part. However, Scikit-learn recently released a ColumnTransformer object that can apply different transformations to the columns of a Pandas DataFrame object.\n",
    "\n",
    "In our case, we can use it to apply one-hot encoding to the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29905cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Handle categorical variables\n",
    "cat_columns = ['yr', 'workingday', 'holiday', 'weekday', 'season', 'weathersit']\n",
    "cat_transformer = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', cat_transformer, cat_columns)\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c993a44",
   "metadata": {},
   "source": [
    "In this code, we **first list the categorical columns in a cat_columns** variable and create the OneHotEncoder() object. This time, we **specify sparse=False when creating the encoder to get Numpy arrays instead of sparse matrices**. We then **create the ColumnTransformer object and specify the different transformations** - one in our case - by defining (name, transformer, vars) triplets. We pass the list of categorical variables with the one-hot encoder and tell the object to **leave the other columns unchanged by setting its remainder attribute to 'passthrough'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5aa010b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 0.00e+00, 1.00e+00, ..., 8.06e-01, 1.60e-01, 3.31e+02],\n",
       "       [1.00e+00, 0.00e+00, 1.00e+00, ..., 6.96e-01, 2.49e-01, 1.31e+02],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, ..., 4.37e-01, 2.48e-01, 1.20e+02],\n",
       "       ...,\n",
       "       [0.00e+00, 1.00e+00, 1.00e+00, ..., 7.53e-01, 1.24e-01, 1.59e+02],\n",
       "       [0.00e+00, 1.00e+00, 1.00e+00, ..., 4.83e-01, 3.51e-01, 3.64e+02],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, ..., 5.78e-01, 1.55e-01, 4.39e+02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = preprocessor.fit_transform(data_df)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7576fbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (731, 24)\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Data type: float64\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', encoded.shape)\n",
    "print('Type:', type(encoded))\n",
    "print('Data type:', encoded.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b5b9b",
   "metadata": {},
   "source": [
    "***Note**: The encoded data is ready for the ML estimators, but not for additional Pandas data manipulation steps since we lost the column names in the conversion. This is why we said above that it can be complex to model sequences of data manipulation steps in Scikit-learn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506d45bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This OneHotEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cat_transformer.get_feature_names()\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae330b83",
   "metadata": {},
   "source": [
    "We get an error saying that **the transformer is not fitted yet**. \n",
    "Just like Pipeline objects, the ColumnTransformer works on copies and not on the original objects directly. To access the copies, we need to use the **named_transformers_** attribute which returns the steps. This is similar to the **named_steps attribute from Pipeline objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071006d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categorical': OneHotEncoder(categorical_features=None, categories=None,\n",
       "        dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "        n_values=None, sparse=False),\n",
       " 'remainder': 'passthrough'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7101bef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_2011', 'x0_2012', 'x1_no', 'x1_yes', 'x2_no', 'x2_yes', 'x3_0',\n",
       "       'x3_1', 'x3_2', 'x3_3', 'x3_4', 'x3_5', 'x3_6', 'x4_fall',\n",
       "       'x4_spring', 'x4_summer', 'x4_winter', 'x5_clear', 'x5_cloudy',\n",
       "       'x5_rainy'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.named_transformers_['categorical'].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446a400",
   "metadata": {},
   "source": [
    "Scikit-learn names the columns by order: x0 corresponds to the first column in cat_columns which is yr.\n",
    "\n",
    "### Issue with missing categories\n",
    "The one-hot encoder creates a new column for each categorical value. A common issue is to have new, previously unknown, categories in the test data. For instance, let's see what happens if we create a new storm category for the weathersit feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6824278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>yr</th>\n",
       "      <th>workingday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>storm</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed    yr workingday holiday  weekday  season  \\\n",
       "0  0.344  0.806       0.16  2011         no      no        6  spring   \n",
       "\n",
       "  weathersit  casual  \n",
       "0      storm     331  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = data_df.iloc[:1].copy()\n",
    "new_data['weathersit'] = 'storm'\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83782d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unknown categories ['storm'] in column 5 during transform\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    preprocessor.transform(new_data)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e618c",
   "metadata": {},
   "source": [
    "The one-hot encoder returns an exception saying that 'storm' is an unknown value. A common practice is to simply ignore unseen values and set all the corresponding one-hot encoded variables to zero i.e. x5_clear, x5_cloudy and x5_rainy.\n",
    "\n",
    "We can specify this behavior by setting the handle_unknown attribute of our OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd497df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 3.44e-01, 8.06e-01, 1.60e-01, 3.31e+02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle categorical variables\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', cat_transformer, cat_columns)\n",
    "], remainder='passthrough')\n",
    "preprocessor.fit_transform(data_df)\n",
    "preprocessor.transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63347981",
   "metadata": {},
   "source": [
    "If we look at the entries with index 17, 18, 19 that correspond to the weathersit variable, we can see that they all have a value of zero.\n",
    "\n",
    "### Ordinal encoding with Scikit-learn\n",
    "Scikit-learn also provides an OrdinalEncoder object to encode ordinal variables. It takes the list of ordinal values and encodes them using a 0 to N integer scale. Let's test it on the weathersit variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c65c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Handle ordinal variables\n",
    "ord_columns = ['weathersit']\n",
    "ord_transformer = OrdinalEncoder(categories=[['clear', 'cloudy', 'rainy']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07d230",
   "metadata": {},
   "source": [
    "In this case, the encoder will simply map clear, cloudy and rainy to respectively 0, 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2db06",
   "metadata": {},
   "source": [
    "### FunctionTransformer object\n",
    "\n",
    "Ordinal and one-hot encoding are two common transformations which have their dedicated Scikit-learn transformers. However, we can also create new transformers with the FunctionTransformer object.\n",
    "\n",
    "For instance, let's create polynomial features with continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9b677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Add polynomial features\n",
    "poly_columns = ['temp', 'hum', 'windspeed']\n",
    "poly_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', FunctionTransformer(lambda X: np.c_[X, X**2, X**3]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f771d",
   "metadata": {},
   "source": [
    "The FunctionTransformer takes a function to apply as a parameter. In the code from above, we create an anonymous one with the lambda notation. The function simply adds the degree 2 and 3 to the input array X with the **np.c_[] concatenation operation.**\n",
    "\n",
    "Note that our **transformer from above is not equivalent to the PolynomialFeatures one which adds all the interaction terms** in addition to the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5bbb88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0',\n",
       " 'x1',\n",
       " 'x2',\n",
       " 'x0^2',\n",
       " 'x0 x1',\n",
       " 'x0 x2',\n",
       " 'x1^2',\n",
       " 'x1 x2',\n",
       " 'x2^2',\n",
       " 'x0^3',\n",
       " 'x0^2 x1',\n",
       " 'x0^2 x2',\n",
       " 'x0 x1^2',\n",
       " 'x0 x1 x2',\n",
       " 'x0 x2^2',\n",
       " 'x1^3',\n",
       " 'x1^2 x2',\n",
       " 'x1 x2^2',\n",
       " 'x2^3']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=3, include_bias=False)\n",
    "polyfeat.fit(data_df[poly_columns])\n",
    "polyfeat.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4f525",
   "metadata": {},
   "source": [
    "As we can see, with the interaction terms, the PolynomialFeatures object creates a total of 19 features instead of just the 9 polynomial ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8f1cf",
   "metadata": {},
   "source": [
    "### Complete pipeline\n",
    "Let's assemble the different transformations into a final ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f717df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lyeso\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "C:\\Users\\lyeso\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(731, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', cat_transformer, cat_columns),\n",
    "    ('ordinal', ord_transformer, ord_columns),\n",
    "    ('poly', poly_transformer, poly_columns)\n",
    "], remainder='drop')\n",
    "\n",
    "encoded = preprocessor.fit_transform(data_df)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e640ae0",
   "metadata": {},
   "source": [
    "This time, we apply the three different transformations and make sure that any additional columns, if any, are dropped by setting remainder to 'drop'.\n",
    "\n",
    "If you execute the code from above, you will probably get a **FutureWarning. Scikit-learn is simply warning us that the default value for one of the object parameters will change in a future release of the library**. We can ignore such warnings by adding a simplefilter using the Python warnings module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "521d4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed6c726",
   "metadata": {},
   "source": [
    "Let's encapsulate our preprocessor with a LinearRegression estimator into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "108dd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa73c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "# Split into train/test sets\n",
    "X = data_df.drop('casual', axis=1)\n",
    "y = data_df.casual\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit/evaluate pipeline\n",
    "pipe.fit(X_tr, y_tr)\n",
    "print('MAE: {:.0f}'.format(MAE(y_te, pipe.predict(X_te)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a4679",
   "metadata": {},
   "source": [
    "### This time, we get a slightly better MAE score than what we obtained in the previous unit with only one-hot encoding: 253 vs. 280.\n",
    "\n",
    "### Summary\n",
    "In this unit, we saw how to perform feature-specific preprocessing steps such as OneHotEncoder, OrdinalEncoder or any FunctionTransformer handmade ones **with the ColumnTransformer object.**\n",
    "\n",
    "In the next unit, we will see how to encapsulate complex transformations into a custom transformer object that can be used with the tools from Scikit-learn.\n",
    "\n",
    "# Custom transformers\n",
    "\n",
    "### This unit is optional\n",
    "In this unit, we will see how to encapsulate **a set of more advanced preprocessing steps done in Pandas into a Scikit-learn custom transformer.**\n",
    "\n",
    "The goal of this unit is to **see how we can implement complex transformations in Scikit-learn. However, custom transformers are quite advanced tools and everything that we will see in this unit can be implemented outside Scikit-learn as a separate Pandas preprocessing step - it's perfectly fine to do it with Pandas.**\n",
    "\n",
    "### Messy bikes data\n",
    "This time, we will work with a variant of the bike sharing data. The data set is similar to the one from the previous unit but has missing values in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8a0fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>yr</th>\n",
       "      <th>workingday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.249</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.248</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clear</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>spring</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.227</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.187</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed      yr workingday holiday  weekday  season  \\\n",
       "0  0.344  0.806        NaN  2011.0         no     NaN      6.0  spring   \n",
       "1  0.363  0.696      0.249  2011.0        NaN      no      0.0  spring   \n",
       "2  0.196  0.437      0.248  2011.0        yes      no      NaN     NaN   \n",
       "3    NaN  0.590      0.160  2011.0        yes      no      2.0  spring   \n",
       "4  0.227  0.437      0.187  2011.0        yes      no      3.0  spring   \n",
       "\n",
       "  weathersit  casual  \n",
       "0     cloudy     331  \n",
       "1     cloudy     131  \n",
       "2      clear     120  \n",
       "3        NaN     108  \n",
       "4      clear      82  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('messy-bikes.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93eba64",
   "metadata": {},
   "source": [
    "Note that the year yr and weekday values are encoded as floating point numbers instead of integers. We will also need to fix this.\n",
    "\n",
    "To get a sense of the proportion of NaN entries, let's run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acd89ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp          0.095759\n",
       "hum           0.109439\n",
       "windspeed     0.112175\n",
       "yr            0.087551\n",
       "workingday    0.088919\n",
       "holiday       0.102599\n",
       "weekday       0.097127\n",
       "season        0.102599\n",
       "weathersit    0.103967\n",
       "casual        0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad0156d",
   "metadata": {},
   "source": [
    "### Custom preprocessing\n",
    "Let's write a preprocess_f(df) function to perform the necessary preprocessing steps. To avoid any issues, we will work on a copy of the df DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d69cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>yr_2011</th>\n",
       "      <th>yr_2012</th>\n",
       "      <th>yr_missing</th>\n",
       "      <th>workingday_missing</th>\n",
       "      <th>workingday_no</th>\n",
       "      <th>workingday_yes</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday_missing</th>\n",
       "      <th>season_fall</th>\n",
       "      <th>season_missing</th>\n",
       "      <th>season_spring</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "      <th>weathersit_clear</th>\n",
       "      <th>weathersit_cloudy</th>\n",
       "      <th>weathersit_missing</th>\n",
       "      <th>weathersit_rainy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.188969</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495543</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       temp    hum  windspeed  casual  yr_2011  yr_2012  yr_missing  \\\n",
       "0  0.344000  0.806   0.188969     331        1        0           0   \n",
       "1  0.363000  0.696   0.249000     131        1        0           0   \n",
       "2  0.196000  0.437   0.248000     120        1        0           0   \n",
       "3  0.495543  0.590   0.160000     108        1        0           0   \n",
       "4  0.227000  0.437   0.187000      82        1        0           0   \n",
       "\n",
       "   workingday_missing  workingday_no  workingday_yes  ...  weekday_missing  \\\n",
       "0                   0              1               0  ...                0   \n",
       "1                   1              0               0  ...                0   \n",
       "2                   0              0               1  ...                1   \n",
       "3                   0              0               1  ...                0   \n",
       "4                   0              0               1  ...                0   \n",
       "\n",
       "   season_fall  season_missing  season_spring  season_summer  season_winter  \\\n",
       "0            0               0              1              0              0   \n",
       "1            0               0              1              0              0   \n",
       "2            0               1              0              0              0   \n",
       "3            0               0              1              0              0   \n",
       "4            0               0              1              0              0   \n",
       "\n",
       "   weathersit_clear  weathersit_cloudy  weathersit_missing  weathersit_rainy  \n",
       "0                 0                  1                   0                 0  \n",
       "1                 0                  1                   0                 0  \n",
       "2                 1                  0                   0                 0  \n",
       "3                 0                  0                   1                 0  \n",
       "4                 1                  0                   0                 0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_f(df):\n",
    "    # Work on a copy\n",
    "    df = df.copy()\n",
    "\n",
    "    # Missing values in continuous features\n",
    "    cont_vars = ['temp', 'hum', 'windspeed']\n",
    "    for c in cont_vars:\n",
    "        df[c] = df[c].fillna(df[c].mean()) # replace by mean\n",
    "\n",
    "    # Explicitely convert to string values\n",
    "    to_convert = ['yr', 'weekday']\n",
    "    convert_f = lambda x: str(int(x)) if not np.isnan(x) else np.nan\n",
    "    df[to_convert] = df[to_convert].applymap(convert_f)\n",
    "\n",
    "    # .. in categorical ones: create 'missing' category\n",
    "    cat_vars = ['yr', 'workingday', 'holiday', 'weekday', 'season', 'weathersit']\n",
    "    df[cat_vars] = df[cat_vars].fillna('missing')\n",
    "\n",
    "    # One-hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "preprocessed = preprocess_f(data_df)\n",
    "preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d8a74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['temp', 'hum', 'windspeed', 'casual', 'yr_2011', 'yr_2012',\n",
       "       'yr_missing', 'workingday_missing', 'workingday_no', 'workingday_yes',\n",
       "       'holiday_missing', 'holiday_no', 'holiday_yes', 'weekday_0',\n",
       "       'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5',\n",
       "       'weekday_6', 'weekday_missing', 'season_fall', 'season_missing',\n",
       "       'season_spring', 'season_summer', 'season_winter', 'weathersit_clear',\n",
       "       'weathersit_cloudy', 'weathersit_missing', 'weathersit_rainy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618e678",
   "metadata": {},
   "source": [
    "As we have seen in the last unit, **we can encapsulate such preprocessing functions into a FunctionTransformer object which can then be used with Scikit-learn tools such as pipelines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5098e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>yr_2011</th>\n",
       "      <th>yr_2012</th>\n",
       "      <th>yr_missing</th>\n",
       "      <th>workingday_missing</th>\n",
       "      <th>workingday_no</th>\n",
       "      <th>workingday_yes</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday_missing</th>\n",
       "      <th>season_fall</th>\n",
       "      <th>season_missing</th>\n",
       "      <th>season_spring</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "      <th>weathersit_clear</th>\n",
       "      <th>weathersit_cloudy</th>\n",
       "      <th>weathersit_missing</th>\n",
       "      <th>weathersit_rainy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.188969</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495543</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       temp    hum  windspeed  casual  yr_2011  yr_2012  yr_missing  \\\n",
       "0  0.344000  0.806   0.188969     331        1        0           0   \n",
       "1  0.363000  0.696   0.249000     131        1        0           0   \n",
       "2  0.196000  0.437   0.248000     120        1        0           0   \n",
       "3  0.495543  0.590   0.160000     108        1        0           0   \n",
       "4  0.227000  0.437   0.187000      82        1        0           0   \n",
       "\n",
       "   workingday_missing  workingday_no  workingday_yes  ...  weekday_missing  \\\n",
       "0                   0              1               0  ...                0   \n",
       "1                   1              0               0  ...                0   \n",
       "2                   0              0               1  ...                1   \n",
       "3                   0              0               1  ...                0   \n",
       "4                   0              0               1  ...                0   \n",
       "\n",
       "   season_fall  season_missing  season_spring  season_summer  season_winter  \\\n",
       "0            0               0              1              0              0   \n",
       "1            0               0              1              0              0   \n",
       "2            0               1              0              0              0   \n",
       "3            0               0              1              0              0   \n",
       "4            0               0              1              0              0   \n",
       "\n",
       "   weathersit_clear  weathersit_cloudy  weathersit_missing  weathersit_rainy  \n",
       "0                 0                  1                   0                 0  \n",
       "1                 0                  1                   0                 0  \n",
       "2                 1                  0                   0                 0  \n",
       "3                 0                  0                   1                 0  \n",
       "4                 1                  0                   0                 0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "preprocessor = FunctionTransformer(preprocess_f, validate=False)\n",
    "preprocessed = preprocessor.fit_transform(data_df)\n",
    "preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f9929",
   "metadata": {},
   "source": [
    "As we saw in the last unit, Scikit-learn transformers work with Numpy arrays and not Pandas DataFrames as in our preprocess_f() function. To avoid any implicit conversion, we need to set the validate parameter to False.\n",
    "\n",
    "### FunctionTransformer limitations\n",
    "The FunctionTransformer has an important limitation: **it can only encapsulate stateless transformations.** For instance, let's see what happens if we pass a single row to the preprocessor fitted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c29304d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>yr_2011</th>\n",
       "      <th>workingday_no</th>\n",
       "      <th>holiday_missing</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>season_spring</th>\n",
       "      <th>weathersit_cloudy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed  casual  yr_2011  workingday_no  holiday_missing  \\\n",
       "0  0.344  0.806        NaN     331        1              1                1   \n",
       "\n",
       "   weekday_6  season_spring  weathersit_cloudy  \n",
       "0          1              1                  1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.transform(data_df.iloc[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4c17c",
   "metadata": {},
   "source": [
    "**This time, the output only has 10 columns instead of 30.** This is because the get_dummies() call inside our preprocess_f() function creates a column for each categorical value. In this case, we only pass a single data point so get_dummies() creates a single column for each categorical variable ex. only weathersit_cloudy since weathersit='cloudy' for this first entry, but no weathersit_rainy as above.\n",
    "\n",
    "### TransformerMixin object\n",
    "To fix this, we need to create a custom transformer that saves the column names during fitting. This can be done by defining a subclass of the Scikit-learn BaseEstimator and TransformerMixin classes and by implementing the __ init__(), fit() and transform() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "737b64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PandasPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, preprocess_f):\n",
    "        self.preprocess_f = preprocess_f\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        # Check that we get a DataFrame\n",
    "        assert type(X_df) == pd.DataFrame\n",
    "\n",
    "        # Preprocess data\n",
    "        X_preprocessed = self.preprocess_f(X_df)\n",
    "\n",
    "        # Save columns names/order for inference time\n",
    "        self.columns_ = X_preprocessed.columns\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        # Check that we get a DataFrame\n",
    "        assert type(X_df) == pd.DataFrame\n",
    "\n",
    "        # Preprocess data\n",
    "        X_preprocessed = self.preprocess_f(X_df)\n",
    "\n",
    "        # Make sure to have the same features\n",
    "        X_reindexed = X_preprocessed.reindex(columns=self.columns_, fill_value=0)\n",
    "\n",
    "        return X_reindexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f2d7e",
   "metadata": {},
   "source": [
    "In this implementation, we create a **PandasPreprocessor transformer that takes a preprocessing preprocess_f function and saves it.**\n",
    "\n",
    "In its fit() method, we check that the input is a DataFrame object and preprocess it with the function. We then save the columns in a new columns_ attribute that we use in the transform() method to make sure that the transformed output has the same set of columns as the input data.\n",
    "\n",
    "Concretely, this is done with a simple df.reindex(columns, fill_value=0) operation which reindexes the DataFrame df such that it has the same columns as columns and in the same order. If the column is missing, it simply creates it and set its values to zeros.\n",
    "\n",
    "Let's see if this fixes our issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b8f8711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>yr_2011</th>\n",
       "      <th>yr_2012</th>\n",
       "      <th>yr_missing</th>\n",
       "      <th>workingday_missing</th>\n",
       "      <th>workingday_no</th>\n",
       "      <th>workingday_yes</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday_missing</th>\n",
       "      <th>season_fall</th>\n",
       "      <th>season_missing</th>\n",
       "      <th>season_spring</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "      <th>weathersit_clear</th>\n",
       "      <th>weathersit_cloudy</th>\n",
       "      <th>weathersit_missing</th>\n",
       "      <th>weathersit_rainy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed  casual  yr_2011  yr_2012  yr_missing  \\\n",
       "0  0.344  0.806        NaN     331        1        0           0   \n",
       "\n",
       "   workingday_missing  workingday_no  workingday_yes  ...  weekday_missing  \\\n",
       "0                   0              1               0  ...                0   \n",
       "\n",
       "   season_fall  season_missing  season_spring  season_summer  season_winter  \\\n",
       "0            0               0              1              0              0   \n",
       "\n",
       "   weathersit_clear  weathersit_cloudy  weathersit_missing  weathersit_rainy  \n",
       "0                 0                  1                   0                 0  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = PandasPreprocessor(preprocess_f)\n",
    "preprocessor.fit(data_df)\n",
    "preprocessor.transform(data_df.iloc[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c674e7a",
   "metadata": {},
   "source": [
    "This time, we get a DataFrame with the correct columns, but the missing entry in windspeed wasn't replaced by the feature mean.\n",
    "\n",
    "Again, the issue comes from our **preprcess_f() implementation which is stateless** - missing values are replaced by the mean of the current DataFrame which, in this case, contains a single entry. Since the mean of NaN is NaN, it didn't impute the missing value. To solve the issue, **we need to store the train mean in the fit() step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78d6601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.cat_vars_ = ['yr', 'workingday', 'holiday', 'weekday', 'season', 'weathersit']\n",
    "        self.cont_vars_ = ['temp', 'hum', 'windspeed']\n",
    "        self.to_convert_ = ['yr', 'weekday']\n",
    "\n",
    "    def preprocess_f(self, X_df, train_mean):\n",
    "        # Work on a copy\n",
    "        X_df = X_df.copy()\n",
    "\n",
    "        # Missing values in continuous features\n",
    "        for c in self.cont_vars_:\n",
    "            X_df[c] = X_df[c].fillna(train_mean[c])\n",
    "\n",
    "        # Explicitely convert to string values\n",
    "        convert_f = lambda x: str(int(x)) if not np.isnan(x) else np.nan\n",
    "        X_df[self.to_convert_] = X_df[self.to_convert_].applymap(convert_f)\n",
    "\n",
    "        # .. in categorical ones: create 'missing' category\n",
    "        X_df[self.cat_vars_] = X_df[self.cat_vars_].fillna('missing')\n",
    "\n",
    "        # One-hot encoding\n",
    "        X_df = pd.get_dummies(X_df)\n",
    "\n",
    "        return X_df\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        # Check that we get a DataFrame\n",
    "        assert type(X_df) == pd.DataFrame\n",
    "\n",
    "        # Save train mean for continuous variables\n",
    "        self.train_mean_ = X_df[self.cont_vars_].mean()\n",
    "\n",
    "        # Preprocess data\n",
    "        X_preprocessed = self.preprocess_f(X_df, self.train_mean_)\n",
    "\n",
    "        # Save columns names/order for inference time\n",
    "        self.columns_ = X_preprocessed.columns\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        # Check that we get a DataFrame\n",
    "        assert type(X_df) == pd.DataFrame\n",
    "\n",
    "        # Preprocess data\n",
    "        X_preprocessed = self.preprocess_f(X_df, self.train_mean_)\n",
    "\n",
    "        # Make sure to have the same features\n",
    "        X_reindexed = X_preprocessed.reindex(columns=self.columns_, fill_value=0)\n",
    "\n",
    "        return X_reindexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15cf16d",
   "metadata": {},
   "source": [
    "Let's look at the main differences with our previous implementation. First, the preprcess_f function and the list of columns are now part of the object. We also pass a train_mean argument to the function. Those values are computed during training time in the fit() step and stored as a train_mean_ attribute. Finally, in the transform() step, we reuse our preprocessing function with the train mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab52dd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>yr_2011</th>\n",
       "      <th>yr_2012</th>\n",
       "      <th>yr_missing</th>\n",
       "      <th>workingday_missing</th>\n",
       "      <th>workingday_no</th>\n",
       "      <th>workingday_yes</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday_missing</th>\n",
       "      <th>season_fall</th>\n",
       "      <th>season_missing</th>\n",
       "      <th>season_spring</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "      <th>weathersit_clear</th>\n",
       "      <th>weathersit_cloudy</th>\n",
       "      <th>weathersit_missing</th>\n",
       "      <th>weathersit_rainy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.188969</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed  casual  yr_2011  yr_2012  yr_missing  \\\n",
       "0  0.344  0.806   0.188969     331        1        0           0   \n",
       "\n",
       "   workingday_missing  workingday_no  workingday_yes  ...  weekday_missing  \\\n",
       "0                   0              1               0  ...                0   \n",
       "\n",
       "   season_fall  season_missing  season_spring  season_summer  season_winter  \\\n",
       "0            0               0              1              0              0   \n",
       "\n",
       "   weathersit_clear  weathersit_cloudy  weathersit_missing  weathersit_rainy  \n",
       "0                 0                  1                   0                 0  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = PandasPreprocessor()\n",
    "preprocessor.fit(data_df)\n",
    "preprocessor.transform(data_df.iloc[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f7ef9",
   "metadata": {},
   "source": [
    "The windspeed value now corresponds to the mean wind speed computed during the fit() call.\n",
    "\n",
    "### Complete Pipeline\n",
    "Let's build a final pipeline with our new PandasPreprocessor custom transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe12e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Use our custom transformer in a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', PandasPreprocessor()),\n",
    "    ('estimator', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce685b4d",
   "metadata": {},
   "source": [
    "We can now evaluate the pipe object as if it was a standard estimator. We will use the train/test set methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "593d2b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "# Split data\n",
    "X = data_df.drop('casual', axis=1)\n",
    "y = data_df.casual\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Evaluate estimator\n",
    "pipe.fit(X_tr, y_tr)\n",
    "print('MAE: {:.0f}'.format(MAE(y_te, pipe.predict(X_te))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc48fcc",
   "metadata": {},
   "source": [
    "It's important to understand that **our implementation handles the separation between train and test sets.** The mean values are computed on the train set X_tr and used to replace missing values in both sets.\n",
    "\n",
    "This time, we get a slightly larger MAE score than in the previous units 295 vs. 280. The difference is due to the missing values in the data - our estimator was not able to perfectly recover the information loss!\n",
    "\n",
    "### Summary\n",
    "In this unit, we experimented with **custom transformers from Scikit-learn and used them to encapsulate a complex set of Pandas preprocessing steps.**\n",
    "\n",
    "It's interesting to note that **Scikit-learn transformers can only transform the features column axis**, but not the data points row axis. For example, we **cannot create transformers that drop data points as it's done with e.g. outliers removal**. In the next unit, we will see how to do this by defining **custom estimators**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
