{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4450ba1a",
   "metadata": {},
   "source": [
    "# True model and noisy observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dfc77",
   "metadata": {},
   "source": [
    "In this unit, we will see the intuition behind the **bias-variance trade-off which is a different view of the under-/overfitting issue**. This time, we will use a synthetic data set and learn about the concept of true model and noisy observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d760fa",
   "metadata": {},
   "source": [
    "We will now go through an example of under-/overfitting. This time, instead of loading a data set, we will generate one. The idea is to define a function that represents the true model. This is the function that we want to approximate or recover from the data. In practice, this task is difficult because **there is always some noise in the data which comes from errors in the measurements and the effect of unobserved variables.**\n",
    "\n",
    "In this example, we will try to recover a sine curve (our true model) from noisy observations of it. Here is an image of a sample data set of 50 points. We plot the true model (sine curve) in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb604193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6d4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3858/true-model.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3858/true-model.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578e002",
   "metadata": {},
   "source": [
    "Data set inspiration from the book Pattern Recognition and Machine Learning by Christopher Bishop\n",
    "https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book\n",
    "\n",
    "\n",
    "Of course, we only have access to the data points in our machine learning tasks, and our goal is to approximate/recover this true function in red.\n",
    "\n",
    "### Intuition behind bias and variance\n",
    "In this example, we will generate three data sets with different noise values and analyze how a model varies when fitted to them.\n",
    "\n",
    "Let's start with the linear regression model. Here are three data sets and their linear regression fit in red. The dashed line in blue corresponds to the true model (the sine curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4219e43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3859/biasvar-deg1.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3859/biasvar-deg1.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f9c04",
   "metadata": {},
   "source": [
    "It's important to understand that we use the same method to generate the three data sets. First, we create a few points on the sine curve and then add some noise to them. In that sense, these data sets are different observations of the same process - the true model.\n",
    "\n",
    "We can see that the lines in red are poor approximations of the data points. We say that the model has a **high bias** since it's not flexible enough to represent well the pattern in the data, i.e., **it's underfitting**. However, the **three lines are very similar to each other, and we say that the model has a low variance** which is a desired property. We don't want a model that varies widely depending on the data set which would mean that the random noise has a large effect on it.\n",
    "\n",
    "Let's do the same experiment with a polynomial of degree three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f587df12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3860/biasvar-deg3.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3860/biasvar-deg3.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2ec13",
   "metadata": {},
   "source": [
    "This time, the curves in red are good approximations of the data points, and they are very similar to the sine function for this range of data. In that sense, this **model has a low bias. Also, it has a low variance since the red curves are very similar** to each other.\n",
    "\n",
    "Finally, let's try with a polynomial of degree 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b46bf434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3861/biasvar-deg9.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3861/biasvar-deg9.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3dc97",
   "metadata": {},
   "source": [
    "It's interesting to note that the two models from above are both special cases of this polynomial of degree 9, i.e., a polynomial of degree 3 can be seen as a polynomial of degree 9 with all coefficients larger than 3 set to zero. For this reason, this model can fit a broader range of data sets and it has the **lowest bias** among the three models.\n",
    "\n",
    "However, the **three red curves from above are terrible approximations** of the sine curve due to the noise in the data which has a large effect on complex models. In that sense, this model has a **large variance**\n",
    "\n",
    "### Summary\n",
    "The three cases from above illustrate the **bias-variance** trade-off.\n",
    "\n",
    "* Models with a high bias (underfitting) usually have a low variance.\n",
    "* Models with a low bias usually have a high variance (overfitting).\n",
    "\n",
    "In practice, we want to find a model that has both a low bias and a low variance. In the next unit, we will see how to manage this bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd60fa",
   "metadata": {},
   "source": [
    "# The bias-variance decomposition\n",
    "\n",
    "In the last unit, we saw the intuition behind the bias-variance trade-off. We will now discuss in more detail the relationship between the bias and the variance of a model, and its ability to generalize. In particular, we will see the dartboard analogy and learn about the **bias-variance decomposition.**\n",
    "\n",
    "### The dartboard analogy\n",
    "In the last unit, we tried to approximate the sine function between zero and one using three different models, and we saw that there is a relationship between bias and variance. Models with a high bias usually have a low variance, and models with a low bias usually have a high variance.\n",
    "\n",
    "We can illustrate this bias-variance trade-off using the dartboard analogy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4042d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2765/dartboard-analogy.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2765/dartboard-analogy.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38af93",
   "metadata": {},
   "source": [
    "The scenario is very similar to the last unit. We have several data sets that contain different observation of the same phenomenon (the true model). We fit our model to each data set and compute a prediction for a sample test data point. In this dartboard analogy, the black dots on the dartboard correspond to our predictions and the red dot to the test point. Our goal is to be as close as possible to this red dot.\n",
    "\n",
    "The image illustrates two typical scenarios.\n",
    "\n",
    "* A model that is underfitting. Predictions from this model are, on average, far from the target value (high bias), but close to each other (low variance)\n",
    "* A model that is overfitting. Predictions are centered around the target value (low bias), but far from each other (high variance)\n",
    "\n",
    "Ideally, we want to find a model with the right \"amount of complexity\" to minimize this bias-variance trade-off.\n",
    "\n",
    "### The bias-variance decomposition\n",
    "It's difficult to say which model from above is the best. In the first case, most of the error comes from the bias of the model, and in the second case, most of it comes from its variance. There exists an analytical expression of this bias-variance trade-off.\n",
    "\n",
    "$Generalization error =\n",
    "Bias\n",
    "2$\n",
    "+\n",
    "$Variance$\n",
    "+\n",
    "$Irreducible error$\n",
    "\n",
    "This is called the bias-variance decomposition. The formula says that the error is equal to the square of the bias of the model plus its variance. Note that there is also an **irreducible error term which is due to the noise in the observations and the effect of unobserved variables**. It's interesting to note that this term sets a lower bound on the generalization error.\n",
    "\n",
    "Here is a figure that illustrates the equation from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be69d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2764/bias-variance-tradeoff.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2764/bias-variance-tradeoff.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe9d47",
   "metadata": {},
   "source": [
    "Figure inspiration from the book Pattern Recognition and Machine Learning by Christopher Bishop, Chapter 3.2\n",
    "\n",
    "The x-axis corresponds to the complexity of the model. You can think of it as the inverse of the regularization strength \n",
    "α\n",
    ". The idea is that the variance of the model (in red) increases with the model complexity while the square of its bias (in blue) decreases. The generalization error is the sum of the two curves plus the irreducible error term.\n",
    "\n",
    "### Summary\n",
    "Minimizing the **generalization error** is one of the main goals of machine learning, and there are many ways to do this.\n",
    "\n",
    "* By tuning the complexity of the model using grid search\n",
    "* By reducing the bias or the variance directly\n",
    "\n",
    "We will see an example of this second option later in this course with **random forests**. In short, this model reduces the variance of another model called a **decision tree** by averaging the output of many different instances of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124be4cf",
   "metadata": {},
   "source": [
    "# Grid search using ParameterGrid\n",
    "\n",
    "We already saw in the last course how to tune a **hyperparameter using grid search**. The **for loop approach worked well in the cases that we saw**, but **doesn't scale well to models with multiple hyperparameters and more complex grids**.\n",
    "\n",
    "In this unit, we will see how to solve this with the $ParameterGrid$ object from Scikit-learn and try to improve our k-NN model on the heart disease data set.\n",
    "\n",
    "### Fit a k-NN classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aafb8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>thalach</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>ca</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>150</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>108</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>presence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>129</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>presence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>187</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>172</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>absence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  trestbps  chol  thalach  oldpeak  ca   disease\n",
       "0   63       145   233      150      2.3   0   absence\n",
       "1   67       160   286      108      1.5   3  presence\n",
       "2   67       120   229      129      2.6   2  presence\n",
       "3   37       130   250      187      3.5   0   absence\n",
       "4   41       130   204      172      1.4   0   absence"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('heart-numerical.csv')\n",
    "\n",
    "# Create X/y arrays\n",
    "X = data_df.drop('disease', axis=1).values\n",
    "y = data_df.disease.values\n",
    "\n",
    "# First five rows\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28a7e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f01cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a k-NN classifier with default values\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Fit to train data\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluate on test set\n",
    "accuracy = pipe.score(X_te, y_te)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85017d9",
   "metadata": {},
   "source": [
    "### Grid search for multiple parameters\n",
    "Let's try to improve our k-NN classifier by tuning some hyperparameters from the KNeighborsClassifier object.\n",
    "\n",
    "* n_neighbors - the number of neighbors.\n",
    "* p - the distance metric. Scikit-learn implements the \n",
    "L\n",
    "1\n",
    " and \n",
    "L\n",
    "2\n",
    " ones.\n",
    "* weights - The weighting function for the majority vote.\n",
    "\n",
    "When doing the **majority vote, the classifier can use a weighting function**. By default, all points have the same weight. This corresponds to the **'uniform' strategy**. However, we can also **give more weights to closer data points**. For instance, the **'distance' strategy assigns a weight inversely proportional to their distance.**\n",
    "\n",
    "Let's define a list of values for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bba7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a set of reasonable values\n",
    "k_values = np.arange(1, 21) # 1, 2, 3, .., 20\n",
    "weights_functions = ['uniform', 'distance']\n",
    "distance_types = [1, 2] # L1, L2 distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220aac4",
   "metadata": {},
   "source": [
    "Using our for loop strategy, we can test the $20 * 2 *2 = 80$ combinations by nesting three for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a3bddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-NN classifier\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Save accuracy on test set\n",
    "test_scores = []\n",
    "\n",
    "# Grid search\n",
    "for k in k_values:\n",
    "    for f in weights_functions:\n",
    "        for d in distance_types:\n",
    "            # Set hyperparameters\n",
    "            pipe.set_params(\n",
    "                knn__n_neighbors=k, knn__weights=f, knn__p=d)\n",
    "\n",
    "            # Fit a k-NN classifier\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            accuracy = pipe.score(X_te, y_te)\n",
    "\n",
    "            # Save accuracy\n",
    "            test_scores.append({\n",
    "                'knn__n_neighbors': k,\n",
    "                'knn__weights': f,\n",
    "                'knn__p': d,\n",
    "                'accuracy': accuracy\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9671d28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>knn__n_neighbors</th>\n",
       "      <th>knn__p</th>\n",
       "      <th>knn__weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.813187</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.813187</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  knn__n_neighbors  knn__p knn__weights\n",
       "14  0.813187                 4       1     distance\n",
       "28  0.813187                 8       1      uniform\n",
       "12  0.802198                 4       1      uniform\n",
       "32  0.802198                 9       1      uniform\n",
       "30  0.802198                 8       1     distance"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with test scores\n",
    "scores_df = pd.DataFrame(test_scores)\n",
    "\n",
    "# Top five scores\n",
    "scores_df.sort_values(by='accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ffcc5",
   "metadata": {},
   "source": [
    "As we can see, it's possible to achieve an accuracy of 80% using the \n",
    "$L\n",
    "1$\n",
    " distance metric. However, the code from above can quickly become complex if we have more hyperparameters to tune or if there are several different grids to evaluate.\n",
    "\n",
    "Let's see how to simplify our code.\n",
    "\n",
    "### Grid search using ParameterGrid\n",
    "\n",
    "Scikit-learn implements a ParameterGrid object to define grids of parameters for our grid search. It takes a dictionary of (parameter, values) pairs. Let's create a grid for our example from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de71d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations: 80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define a grid of values\n",
    "grid = ParameterGrid({\n",
    "    'knn__n_neighbors': k_values,\n",
    "    'knn__weights': weights_functions,\n",
    "    'knn__p': distance_types\n",
    "})\n",
    "\n",
    "# Print the number of combinations\n",
    "print('Number of combinations:', len(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bee1e",
   "metadata": {},
   "source": [
    "This grid variable represents all the combinations of parameters, and we can use it as a list. For instance, we print the total number of combinations using the len() function. We can also use it to iterate through each combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b05af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 1, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 1, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 1, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 1, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 2, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 2, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 2, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 2, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 3, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 3, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 3, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 3, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 4, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 4, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 4, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 4, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 5, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 5, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 6, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 6, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 6, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 6, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 7, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 7, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 7, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 7, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 8, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 8, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 8, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 8, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 9, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 9, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 9, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 9, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 10, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 10, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 11, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 11, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 11, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 11, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 12, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 12, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 12, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 12, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 13, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 13, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 13, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 13, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 14, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 14, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 14, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 14, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 15, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 15, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 15, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 15, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 16, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 16, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 16, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 16, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 17, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 17, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 17, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 17, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 18, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 18, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 18, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 18, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 19, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 19, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 19, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 19, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 20, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 20, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "{'knn__n_neighbors': 20, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "{'knn__n_neighbors': 20, 'knn__p': 2, 'knn__weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "for params_dict in grid:\n",
    "    print(params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95803e54",
   "metadata": {},
   "source": [
    "At each iteration, the params_dict variable contains a dictionary with the current combination of values. The idea is to initialize the pipeline with this dictionary of values using the set_params() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e8cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-NN classifier\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Save accuracy on test set\n",
    "test_scores = []\n",
    "\n",
    "for params_dict in grid:\n",
    "    # Set parameters\n",
    "    pipe.set_params(**params_dict)\n",
    "\n",
    "    # Fit a k-NN classifier\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    # Save accuracy on test set\n",
    "    params_dict['accuracy'] = pipe.score(X_te, y_te)\n",
    "\n",
    "    # Save result\n",
    "    test_scores.append(params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef8f45",
   "metadata": {},
   "source": [
    "This time, we need a single for loop to iterate through each combination of parameters. At each iteration, we set the parameters of our pipeline using the set_params() function and the ****kwargs** syntax which is a way to work with keyword arguments. In short, the idea is to set the arguments of a function using a dictionary of (keyword, value) pairs. In our case, this dictionary corresponds to the params_dict variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab849606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value of params_dict for the first combination\n",
    "params_dict = {'knn__n_neighbors': 1, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
    "\n",
    "# Setting the parameters using the **kwargs syntax\n",
    "pipe.set_params(**params_dict)\n",
    "\n",
    "# .. is equivalent to\n",
    "pipe.set_params(knn__n_neighbors=1, knn__p=1, knn__weights='uniform');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98698496",
   "metadata": {},
   "source": [
    "Finally, we fit the k-NN classifier, save its accuracy in the params_dict dictionary and add it to the test_scores list.\n",
    "\n",
    "Again, we can use this list to create a DataFrame with the results and print the top five scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "373ae3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>knn__n_neighbors</th>\n",
       "      <th>knn__p</th>\n",
       "      <th>knn__weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.813187</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.813187</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  knn__n_neighbors  knn__p knn__weights\n",
       "28  0.813187                 8       1      uniform\n",
       "13  0.813187                 4       1     distance\n",
       "32  0.802198                 9       1      uniform\n",
       "12  0.802198                 4       1      uniform\n",
       "37  0.802198                10       1     distance"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with test scores\n",
    "scores_df = pd.DataFrame(test_scores)\n",
    "\n",
    "# Top five scores\n",
    "scores_df.sort_values(by='accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570cb3fe",
   "metadata": {},
   "source": [
    "### Multiple grids\n",
    "One of the advantages of the ParameterGrid object is that we can define many grids of parameters by passing a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d18d26fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'knn__n_neighbors': 2, 'knn__p': 1},\n",
       " {'knn__n_neighbors': 2, 'knn__p': 2},\n",
       " {'knn__n_neighbors': 3, 'knn__p': 1},\n",
       " {'knn__n_neighbors': 3, 'knn__p': 2},\n",
       " {'knn__p': 1, 'knn__weights': 'uniform'},\n",
       " {'knn__p': 1, 'knn__weights': 'distance'},\n",
       " {'knn__p': 2, 'knn__weights': 'uniform'},\n",
       " {'knn__p': 2, 'knn__weights': 'distance'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define two grids\n",
    "grid = ParameterGrid([{\n",
    "    'knn__n_neighbors': [2, 3],\n",
    "    'knn__p': [1, 2]\n",
    "},{\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]\n",
    "}])\n",
    "\n",
    "# List combinations\n",
    "list(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775dbbe3",
   "metadata": {},
   "source": [
    "The first four elements correspond to the combinations from the first grid and the last four to the combinations from the second grid.\n",
    "\n",
    "There is a small issue in this case. The first grid doesn't specify the weights parameter and the second grid doesn't specify the n_neighbors one. Hence, we can get unexpected results since we don't set these parameters. One solution is to assign a default value to each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a672b048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'knn__n_neighbors': 2, 'knn__p': 1, 'knn__weights': 'uniform'},\n",
       " {'knn__n_neighbors': 2, 'knn__p': 2, 'knn__weights': 'uniform'},\n",
       " {'knn__n_neighbors': 3, 'knn__p': 1, 'knn__weights': 'uniform'},\n",
       " {'knn__n_neighbors': 3, 'knn__p': 2, 'knn__weights': 'uniform'},\n",
       " {'knn__n_neighbors': 5, 'knn__p': 1, 'knn__weights': 'uniform'},\n",
       " {'knn__n_neighbors': 5, 'knn__p': 1, 'knn__weights': 'distance'},\n",
       " {'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'uniform'},\n",
       " {'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'distance'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define two grids\n",
    "grid = ParameterGrid([{\n",
    "    'knn__n_neighbors': [2, 3],\n",
    "    'knn__weights': ['uniform'], # Default value: uniform\n",
    "    'knn__p': [1, 2]\n",
    "},{\n",
    "    'knn__n_neighbors': [5], # Default value: 5\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]\n",
    "}])\n",
    "\n",
    "# List combinations\n",
    "list(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3892fb8",
   "metadata": {},
   "source": [
    "The grid still has eight combinations, but this time, they all specify a value for each parameter.\n",
    "\n",
    "### Optional steps\n",
    "\n",
    "It's also possible to disable a step by setting it to None. For instance, we can try fitting a k-NN model without standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85d18ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'knn__n_neighbors': 5, 'scaler': None},\n",
       " {'knn__n_neighbors': 5,\n",
       "  'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
       " {'knn__n_neighbors': 10, 'scaler': None},\n",
       " {'knn__n_neighbors': 10,\n",
       "  'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
       " {'knn__n_neighbors': 15, 'scaler': None},\n",
       " {'knn__n_neighbors': 15,\n",
       "  'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid with optional steps\n",
    "grid = ParameterGrid({\n",
    "    'scaler': [None, StandardScaler()],\n",
    "    'knn__n_neighbors': [5, 10, 15],\n",
    "})\n",
    "\n",
    "# List combinations\n",
    "list(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a6c5de",
   "metadata": {},
   "source": [
    "This time, we're not setting a parameter, but the step itself. Scikit-learn will disable the 'scaler' step when it gets a None value and will use the StandardScaler object in the other cases.\n",
    "\n",
    "### Summary\n",
    "In this unit, we saw how to test complex grids of hyperparameter combinations with ParameterGrid. The goal of the next exercise is to experiment with hyperparameter tuning and build a k-NN classifier to recognize images from the CIFAR-10 data set\n",
    "\n",
    "# Exercise - CIFAR-10 classification with k-NN\n",
    "See 4-3-1\n",
    "\n",
    "### Parallelized execution\n",
    "\n",
    "Nowadays, processors can make multiple computations in parallel. Scikit-learn k-NN implementation can leverage this to improve the speed of the classifier. The KNeighborsClassifier estimator provides an n_jobs parameters for defining this number of parallel processes. **You can set it to -1 to let Scikit-learn** use all available processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "082aeebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create k-NN classifier\n",
    "KNeighborsClassifier(\n",
    "    n_jobs=-1 # As many parallel jobs as possible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcfbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
