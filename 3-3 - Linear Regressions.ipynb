{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electoral-music",
   "metadata": {},
   "source": [
    "# Multiple linear regressions\n",
    "\n",
    "In the previous subject, we learned about simple linear regressions which use the equation of a line to model the relationship between a single feature and the target variable. We will now see how to generalize this idea to multiples features.\n",
    "\n",
    "At the end of this unit, **you should be able to fit linear regressions to data sets with multiple features using the lstsq() function from the Scipy library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chubby-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_df shape: (50, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tv</th>\n",
       "      <th>web</th>\n",
       "      <th>radio</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.916</td>\n",
       "      <td>1.689</td>\n",
       "      <td>0.208</td>\n",
       "      <td>1.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.359</td>\n",
       "      <td>1.706</td>\n",
       "      <td>1.071</td>\n",
       "      <td>4.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.261</td>\n",
       "      <td>2.538</td>\n",
       "      <td>2.438</td>\n",
       "      <td>3.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.682</td>\n",
       "      <td>2.092</td>\n",
       "      <td>1.283</td>\n",
       "      <td>5.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.736</td>\n",
       "      <td>1.660</td>\n",
       "      <td>1.800</td>\n",
       "      <td>5.993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tv    web  radio  sales\n",
       "0   0.916  1.689  0.208  1.204\n",
       "1   9.359  1.706  1.071  4.800\n",
       "2   5.261  2.538  2.438  3.970\n",
       "3   8.682  2.092  1.283  5.212\n",
       "4  11.736  1.660  1.800  5.993"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('marketing-campaign.csv')\n",
    "print('data_df shape:', data_df.shape) # (50, 4)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-ribbon",
   "metadata": {},
   "source": [
    "### Implementation with Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "innovative-appraisal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (50, 3)\n",
      "y: (50,)\n"
     ]
    }
   ],
   "source": [
    "# Extract input matrix X\n",
    "X = data_df.drop('sales', axis=1).values\n",
    "print('X:', X.shape) # (50, 3)\n",
    "\n",
    "# Extract target vector y\n",
    "y = data_df.sales.values\n",
    "print('y:', y.shape) # (50,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-house",
   "metadata": {},
   "source": [
    "We now want to **find the vector of coefficients** \n",
    "⃗\n",
    "w\n",
    " that minimizes an objective function. In this unit, we will use the lstsq() function from Scipy which computes the least squares solution of the equation \n",
    "A\n",
    "x\n",
    "=\n",
    "b\n",
    ". In our case, \n",
    "A\n",
    ", \n",
    "b\n",
    " and \n",
    "x\n",
    " correspond respectively to \n",
    "X\n",
    ", \n",
    "y\n",
    " and \n",
    "⃗\n",
    "w\n",
    ", and the least squares solution is the vector \n",
    "⃗\n",
    "w\n",
    " that minimizes the squared distances between the two sides of the equation\n",
    "\n",
    "X\n",
    "⃗\n",
    "w\n",
    "=\n",
    "y\n",
    "**In other words, the lstsq() function will return the parameter values that minimize the squares of the difference between the predictions of our linear regression model (without the intercept term)** \n",
    "⃗\n",
    "y\n",
    "p\n",
    "r\n",
    "e\n",
    "d\n",
    "=\n",
    "X\n",
    "⃗\n",
    "w\n",
    " and the target values \n",
    "y\n",
    ".\n",
    "\n",
    "Let's try it with our X and y arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-composite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [0.3958359  0.47521518 0.31040001]\n",
      "RSS: 1.6884039033000033\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import lstsq\n",
    "\n",
    "# Fit a multiple linear regression\n",
    "w, rss, _, _ = lstsq(X, y)\n",
    "print('w:', w) # [ 0.3958359   0.47521518  0.31040001]\n",
    "print('RSS:', rss) # 1.688"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-optimum",
   "metadata": {},
   "source": [
    "The function returns four values. As for now, we will only look at the first two. The first one w is the vector of coefficients (one for each feature) and the second one rss is the residual sum of squares.\n",
    "\n",
    "For reference, **the RSS score of our simple linear regression model was around 15.7. Hence, we gained a lot in accuracy by bringing the two other marketing budgets in the equation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-copper",
   "metadata": {},
   "source": [
    "### Adding the intercept term\n",
    "The code from above computes the optimal solution without the intercept term \n",
    "w\n",
    "0\n",
    ". However, it's possible to make the lstsq() function compute it using a little trick. The idea is to add a column of ones in the matrix \n",
    "X\n",
    ". This column corresponds to the \n",
    "w\n",
    "0\n",
    " element in \n",
    "⃗\n",
    "w\n",
    ".\n",
    "\n",
    "The ones multiply the intercept term \n",
    "w\n",
    "0\n",
    ", and we get the equation of linear regressions from above. To generate this additional column, we can use the ones() function from Numpy. Then, we concatenate it to the X matrix with the Numpy c_ object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moved-article",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  0.916,  1.689,  0.208],\n",
       "       [ 1.   ,  9.359,  1.706,  1.071],\n",
       "       [ 1.   ,  5.261,  2.538,  2.438],\n",
       "       [ 1.   ,  8.682,  2.092,  1.283],\n",
       "       [ 1.   , 11.736,  1.66 ,  1.8  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add a column of ones\n",
    "X1 = np.c_[\n",
    "    np.ones(X.shape[0]), # Vector of ones of shape (n,)\n",
    "    X # X matrix of shape (n,p)\n",
    "]\n",
    "\n",
    "X1[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supported-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, rss, _, _ = lstsq(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "declared-anime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [0.02487092 0.39465146 0.47037002 0.30669954]\n",
      "RSS: 1.6854508680824727\n"
     ]
    }
   ],
   "source": [
    "print('w:', w)\n",
    "print('RSS:', rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-listening",
   "metadata": {},
   "source": [
    "The array w has now four elements. The first one w[0] corresponds to the intercept term and the other three w[1:] to the coefficients. Note that the intercept is close to zero. Hence the RSS score didn't change significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dried-stable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: (50,)\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions\n",
    "y_pred = np.matmul(X1, w)\n",
    "print('y_pred:', y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "respected-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS: 1.685450868082472\n"
     ]
    }
   ],
   "source": [
    "# Verify RSS score\n",
    "def RSS(y, y_pred):\n",
    "    return np.sum(np.square(np.subtract(y, y_pred)))\n",
    "\n",
    "rss = RSS(y, y_pred)\n",
    "print('RSS:', rss) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-hacker",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this unit, we saw how to generalize linear regressions to multiple features, and how to implement them using the lstsq() function from Scipy. In practice, we usually work with higher level tools such as the LinearRegression object from Scikit-learn. However, it's good to know how they work internally.\n",
    "\n",
    "In the next unit, we will learn about the coefficient of determination \n",
    "R\n",
    "2\n",
    " which is a common evaluation metric for linear regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-armor",
   "metadata": {},
   "source": [
    "# R^2 coefficient\n",
    "In this unit, we will learn about the coefficient of determination \n",
    "R\n",
    "2\n",
    " which is a common evaluation metric for linear regressions. It's also the default metric returned by many Scikit-learn objects such as SGDRegressor or HuberRegressor.\n",
    "\n",
    "R\n",
    "2\n",
    " coefficient\n",
    "Unlike cost functions, the coefficient \n",
    "R\n",
    "2\n",
    " **doesn't directly express the error made by our model. In fact, we can interpret it as a comparison between our model and the constant mean baseline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedicated-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('marketing-campaign.csv')\n",
    "X = data_df.drop('sales', axis=1).values\n",
    "y = data_df.sales.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "outside-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS baseline: 100.86060792000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define RSS measure\n",
    "def RSS(y, y_pred):\n",
    "    return np.sum(np.square(np.subtract(y, y_pred)))\n",
    "\n",
    "# RSS of the baseline\n",
    "rss_baseline = RSS(y, y.mean())\n",
    "print('RSS baseline:', rss_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "british-angle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS: 1.6854508680824727\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import lstsq\n",
    "\n",
    "# Fit a multiple linear regression\n",
    "X1 = np.c_[np.ones(X.shape[0]), X]\n",
    "w, model_rss, _, _ = lstsq(X1, y)\n",
    "print('RSS:', model_rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-burlington",
   "metadata": {},
   "source": [
    "As we can see, there is a significant difference in the RSS scores between our baseline and the linear regression. To quantify this difference, we can compute the \n",
    "R\n",
    "2\n",
    " coefficient which is one minus the ratio between the two scores.\n",
    " \n",
    "- If R2= 1, then the ratio is zero which means that the RSS of the model is zero and that it makes no error\n",
    "- If \n",
    "R\n",
    "2\n",
    " is close to \n",
    "1\n",
    ", then the model performs way better than the baseline\n",
    "- If \n",
    "R\n",
    "2\n",
    " is close to \n",
    "0\n",
    ", then the model and the baseline have a similar performance\n",
    "- If \n",
    "R\n",
    "2\n",
    "<\n",
    "0\n",
    ", then it performs worse than the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cooperative-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 coefficient: 0.9832893048848236\n"
     ]
    }
   ],
   "source": [
    "# R^2 coefficient\n",
    "R2 = 1 - (model_rss / rss_baseline)\n",
    "print('R^2 coefficient:', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "static-blanket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 coefficient: 0.8439430385697798\n"
     ]
    }
   ],
   "source": [
    "# R^2 of simple linear regression model\n",
    "R2 = 1 - (15.74 / rss_baseline)\n",
    "print('R^2 coefficient:', R2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-aurora",
   "metadata": {},
   "source": [
    "### Proportion of variance explained\n",
    "\n",
    "*The coefficient of determination \n",
    "R\n",
    "2\n",
    " is the proportion of the variance in the target variable \n",
    "y\n",
    " that is predictable from the input variable(s) \n",
    "x*\n",
    "\n",
    "In other words, it's a measure of how well we can explain, with our model, how the target variable \n",
    "y\n",
    " varies using the input variable(s) \n",
    "x\n",
    ". The idea is that the target values vary around their mean value and we can quantify this variation by summing the squares of the differences between each target value \n",
    "y\n",
    "i\n",
    " and the mean target value \n",
    "¯\n",
    "y\n",
    ".\n",
    "\n",
    "The ratio between these two values correspond to the **proportion of variance left in the data or not explained by the model**. If we subtract this ratio to one, we obtain the proportion of variance explained by our model which is the \n",
    "R\n",
    "2\n",
    " coefficient.\n",
    " \n",
    " \n",
    "### Summary\n",
    "Here are a few takeaways about the \n",
    "R\n",
    "2\n",
    " coefficient.\n",
    "\n",
    "- Intuitively, it measures how well a model performs compared to the constant mean baseline\n",
    "- It is always smaller than or equal to one. Ideally, it should be close to one\n",
    "- The coefficient is defined as the proportion of variance explained by the model\n",
    "\n",
    "So far, we focused on how the linear regression model works, but not on the methods that find the set of optimal parameters \n",
    "⃗\n",
    "w\n",
    ". **In the next unit, we will learn about the ordinary least squares (OLS) method which is the one used by the polyfit() and the lstsq() functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-cookie",
   "metadata": {},
   "source": [
    "# Ordinary least squares\n",
    "\n",
    "The goal of this unit is to get familiar with the topic of cost function optimization. We will start with a review of derivatives and gradients, and see how we can use them to optimize cost functions.\n",
    "\n",
    "In the second part of this unit, we will learn about the ordinary least squares (OLS) method which is the one used by the lstsq() function to compute the least squares solution. We will implement this method using basic Numpy code and get the set of optimal parameters of a linear regression model for the marketing campaign data set.\n",
    "\n",
    "### Cost functions and derivatives\n",
    "\n",
    "Gradient of the cost function\n",
    "Ordinary least squares\n",
    "\n",
    "Solving the equation on the gradient \n",
    "∇\n",
    "f\n",
    "=\n",
    "0\n",
    " only works for a specific class of functions \n",
    "f\n",
    " that are \"flat\" only at their optimal value. The good news is that the RSS cost function of a linear regression model is one of them and we can find the optimal parameters analytically by solving the equation. You can take a look at this post if you want to learn more about the proof.\n",
    "\n",
    "This analytical solution is called the ordinary least squares (OLS) solution. Here is the formula that we get when solving the equation on the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-grass",
   "metadata": {},
   "source": [
    "### OLS on the marketing campaign data set\n",
    "We will now implement the OLS method using the formula from above. Our goal is to find the set of optimal parameters of a linear regression model for the marketing campaign data set.\n",
    "\n",
    "Let's start by loading the data set into an X and a y Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opponent-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('marketing-campaign.csv')\n",
    "X = data_df.drop('sales', axis=1).values\n",
    "y = data_df.sales.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-painting",
   "metadata": {},
   "source": [
    "We can then use the matmul() function from Numpy to compute matrix multiplications and get the transpose \n",
    "X\n",
    "⊺\n",
    " with the T attribute of Numpy arrays. For the matrix inversion \n",
    "(\n",
    "X\n",
    "⊺\n",
    "X\n",
    ")\n",
    "−\n",
    "1\n",
    ", we can use the Numpy inv() function from its linalg module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "another-warrior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [0.02487092 0.39465146 0.47037002 0.30669954]\n"
     ]
    }
   ],
   "source": [
    "# Compute OLS solution\n",
    "XX = np.matmul(X1.T, X1)\n",
    "Xy = np.matmul(X1.T, y)\n",
    "w = np.matmul(np.linalg.inv(XX), Xy)\n",
    "\n",
    "print('w:', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "behavioral-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [0.02487092 0.39465146 0.47037002 0.30669954]\n"
     ]
    }
   ],
   "source": [
    "# Let's do a quick verification using the lstsq() function from Numpy.\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "# Verify with Scipy lstsq\n",
    "w, _, _, _ = lstsq(X1, y)\n",
    "\n",
    "print('w:', w)\n",
    "# Prints: [ 0.02487092  0.39465146  0.47037002  0.30669954]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-programmer",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this unit, we learned about the ordinary least squares (OLS) solution which is the standard way to find the set of parameters for linear regressions. In particular, we saw that it's an analytical solution that minimizes the RSS measure by solving an equation on its gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-indiana",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "final-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('marketing-campaign.csv')\n",
    "X = data_df.drop('sales', axis=1).values\n",
    "y = data_df['sales'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-robin",
   "metadata": {},
   "source": [
    "The Scikit-learn library provides a **LinearRegression object** that uses the OLS method to fit a linear regression model. Just like the HuberRegressor and the SGDRegressor ones, this object **implements the fit() function.** In the Scikit-learn jargon, these objects are estimators and the function is part of the estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "military-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.39465146 0.47037002 0.30669954]\n",
      "Intercept: 0.024870917888195176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Print coefficients\n",
    "print('Coefficients:', lr.coef_)\n",
    "# Prints: [ 0.39465146  0.47037002  0.30669954]\n",
    "\n",
    "print('Intercept:', lr.intercept_)\n",
    "# Prints: 0.02487091788819562"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-warrior",
   "metadata": {},
   "source": [
    "In this code, we start by creating a LinearRegression() object. Then, we call fit() with the X, y variables. This function also relies on the lstsq() function from Scipy to compute the set of optimal parameters. However, note that we don't have to add the additional column of ones to the input matrix X. The LinearRegression() object will automatically compute the intercept term \n",
    "w\n",
    "0\n",
    ". We can access the parameters with the coef_ and the intercept_ attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-occasion",
   "metadata": {},
   "source": [
    "For reference, here are the optimal coefficients that we computed in the last unit with the OLS formula.\n",
    "\n",
    "For reference: scipy lstsq returns\n",
    "w: [ 0.02487092  0.39465146  0.47037002  0.30669954]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-dover",
   "metadata": {},
   "source": [
    "### Predict and score functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "talented-appointment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.24462012, 4.84934038, 4.04266482])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute predictions\n",
    "y_pred = lr.predict(X)\n",
    "y_pred[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "recreational-compound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.24462012, 4.84934038, 4.04266482])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also manually compute these predictions with the matmul() function from Numpy.\n",
    "import numpy as np\n",
    "\n",
    "y_pred = np.matmul(X, lr.coef_) + lr.intercept_\n",
    "y_pred[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "impossible-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.9832893048848236\n"
     ]
    }
   ],
   "source": [
    "# Compute the R2 cofficient\n",
    "R2 = lr.score(X, y)\n",
    "print('R2:', R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-mills",
   "metadata": {},
   "source": [
    "### SGDRegressor\n",
    "The OLS method is the most common way to find the parameters of a linear regression model that minimize the squares of the residuals. However, in the unit about Huber loss, **we saw the SGDRegressor object which implements a variant of the gradient descent (GD) algorithm.**\n",
    "\n",
    "It's important to understand that **this algorithm doesn't compute an analytical solution.** It's an iterative algorithm that tries to get closer to the optimal solution after each iteration. However, unlike the OLS method, gradient descent is very generic and can optimize many different cost functions, e.g., Huber loss.\n",
    "\n",
    "To minimize the squares of the residuals, we can set its loss parameter to squared_loss. By default, it adds a penalization term to the cost function. We will learn more about it later in this course. As for now, we can set it 'none'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afraid-burton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.39968853 0.44409771 0.25894341]\n",
      "Intercept: [0.12807209]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create the SGDRegressor object\n",
    "lr_sgd = SGDRegressor(\n",
    "    loss='squared_loss', # Cost function\n",
    "    penalty='none', # Add a penalty term?\n",
    "    max_iter=1000, # Number of iterations\n",
    "    random_state=0, # The implementation shuffles the data\n",
    "    tol=1e-3 # Tolerance for improvement (stop SGD once loss is below)\n",
    ")\n",
    "\n",
    "# Fit the linear regression model\n",
    "lr_sgd.fit(X, y)\n",
    "\n",
    "# Print coefficients\n",
    "print('Coefficients:', lr_sgd.coef_)\n",
    "\n",
    "print('Intercept:', lr_sgd.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-athletics",
   "metadata": {},
   "source": [
    "The implementation of the SGDRegressor object shuffles the data before running the optimization algorithm. To get the results from above, you should set its random_state parameter to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "applied-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_sgd: 0.9821546772612869\n"
     ]
    }
   ],
   "source": [
    "# Compute R2 coefficient\n",
    "R2_sgd = lr_sgd.score(X, y)\n",
    "print('R2_sgd:', R2_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-drunk",
   "metadata": {},
   "source": [
    "### Huber loss\n",
    "We usually fit linear regressions using the least squares approach, i.e., minimizing the squares of the residuals. However, it's also possible to use other objective functions such as Huber loss.\n",
    "\n",
    "To achieve this, we can create an HuberRegressor object which also implements the estimator interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "controlled-german",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.39172544 0.4788203  0.29315421]\n",
      "Intercept: 0.04586298819194167\n",
      "R^2 coefficient: 0.983070157114285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Create the estimator\n",
    "huber = HuberRegressor(epsilon=1.35)\n",
    "\n",
    "# Fit it to X,y\n",
    "huber.fit(X, y)\n",
    "\n",
    "print('Coefficients:', huber.coef_)\n",
    "\n",
    "\n",
    "print('Intercept:', huber.intercept_)\n",
    "\n",
    "\n",
    "print('R^2 coefficient:', huber.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-screening",
   "metadata": {},
   "source": [
    " this code, we create a HuberRegressor object with a threshold of 1.35 and fit it to the X, y data. We can then access the parameters with the coef_ and the intercept_ attributes. They are a bit different than the ones from above because they optimize another cost function. Finally, we compute the \n",
    "R\n",
    "2\n",
    " coefficient with the score() function.\n",
    " \n",
    "### Summary\n",
    "Let's summarize what we've learned in this unit. Here are a few takeaways.\n",
    "\n",
    "- The Scikit-learn library implements several estimators to fit linear regressions, e.g., the LinearRegression, SGDRegressor and HuberRegressor objects\n",
    "- They implement the estimator API, e.g., the fit(), predict(), score() functions.\n",
    "- Scikit-learn estimators work with 2-dimensional arrays of features X, i.e., unlike the polyfit() function which expects a 1-dimensional input vector x.\n",
    "\n",
    "In the next unit, we will do a summary of the different functions and objects to fit linear regressions, and we will see when to use each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-current",
   "metadata": {},
   "source": [
    "# How to choose the appropriate method?\n",
    "\n",
    "We've seen many different functions and objects to fit linear regressions. Let's do a summary and see when to use each method.\n",
    "\n",
    "### Numpy polyfit\n",
    "The polyfit(x, y, deg) function fits a polynomial of degree deg to a set of x/y points. The function computes the least squares solution which corresponds to a simple linear regression model when deg=1. Later in this course, we will see that this function is a special case of linear regressions with a polynomial basis, and we will see how to implement it with the LinearRegression and the PolynomialFeatures objects from Scikit-learn.\n",
    "\n",
    "**Use the polyfit() function when fitting a polynomial between a single input variable x and an output variable y.**\n",
    "\n",
    "### OLS: Numpy implementation\n",
    "The ordinary least squares (OLS) formula is an analytical solution of linear regressions with the RSS cost function. It's possible to implement the formula with the Numpy matmul() and inv() functions.\n",
    "\n",
    "⃗\n",
    "w\n",
    "=\n",
    "(\n",
    "X\n",
    "⊺\n",
    "X\n",
    ")\n",
    "−\n",
    "1\n",
    "X\n",
    "⊺\n",
    "⃗\n",
    "y\n",
    "\n",
    "**However, in practice, we don't implement this formula by ourselves but use existing optimized implementations.**\n",
    "\n",
    "### OLS: Scipy lstsq\n",
    "There are many ways to implement the OLS solution. You can take a look at this post to learn more about them. The lstsq() function from Scipy uses optimized algorithms to compute it.\n",
    "\n",
    "### Scikit-learn LinearRegression\n",
    "Scikit-learn provides a LinearRegression estimator to compute the OLS solution which uses the lstsq() function internally. It's very convenient to work with this object since it implements the estimator API. For this reason, **we recommend you to use this object to fit linear regressions.**\n",
    "\n",
    "### Scikit-learn HuberRegressor\n",
    "Scikit-learn also provides a HuberRegressor estimator for linear regressions with Huber loss. The object optimizes this cost function using a iterative method called BFGS.\n",
    "\n",
    "**It's a good idea to try fitting a linear regression with Huber loss if you believe that there are outliers in the data.**\n",
    "\n",
    "### Scikit-learn SGDRegressor\n",
    "It's also possible to fit linear regressions with the SGDRegressor estimator from Scikit-learn which implements the **stochastic gradient descent (SGD) algorithm.** In short, it's a variant of the gradient descent (GD) algorithm that **can optimize many different functions, e.g., Huber loss, squared loss.** We will learn more about this algorithm in the next subject.\n",
    "\n",
    "One of its main advantages is its complexity - **it is faster for very large data sets (e.g., millions of data points or tens of thousands of features) and requires less memory.** You can take a look at this tutorial from Scikit-learn if you want to learn more about it.\n",
    "\n",
    "**Use it for linear regressions when it's not possible to use the LinearRegression or the HuberRegressor objects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-speaking",
   "metadata": {},
   "source": [
    "# Ill-conditioning\n",
    "\n",
    "In this unit, we will learn about **ill-conditioning** which can cause numerical issues when computing the ordinary least squares solution (OLS). We will start by describing **collinearity** which is a phenomenon closely related to ill-conditioning. Then, we will see how nearly collinear features can cause numerical issues. Finally, we will experiment with regularization which is a way to handle ill-conditioning.\n",
    "\n",
    "### Collinearity\n",
    "\n",
    "Collinearity (and multicollinearity) happens when there is an **exact linear relationship between one or more features in the input matrix \n",
    "X**\n",
    ". This means that one feature is a combination of the others plus some constant. This may sound abstract, so let's take an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statistical-container",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1964</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2270</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2043</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1508</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     temp  users\n",
       "0  0.1964    120\n",
       "1  0.2000    108\n",
       "2  0.2270     82\n",
       "3  0.2043     88\n",
       "4  0.1508     41"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('bike-sharing.csv')\n",
    "\n",
    "# Create Numpy arrays\n",
    "temp = data_df.temp.values\n",
    "users = data_df.users.values\n",
    "\n",
    "# First five rows\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-press",
   "metadata": {},
   "source": [
    "As we can see, temperatures are not in degrees Celcius or Fahrenheit. In fact, the data set web page states that the original temperatures in degrees Celcius temp_C were rescaled between zero and one using the formula temp=(temp_C+8)/47."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reported-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collinear feature\n",
    "temp_C = 47*temp - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-miller",
   "metadata": {},
   "source": [
    "By construction, **the temp and the temp_C variables are collinear**. The issue with collinear features is that they make the \n",
    "X\n",
    " matrix with the additional column of ones, and its moment matrix \n",
    "X\n",
    "⊺\n",
    "X\n",
    ", rank deficient which means that not all columns are linearly independent. A matrix that is rank-deficient is not invertible. **Hence, the OLS solution does not exist.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strange-alliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create input matrix X\n",
    "X = np.c_[temp, temp_C]\n",
    "\n",
    "# Add a column of ones\n",
    "X1 = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# Compute rank\n",
    "rank = np.linalg.matrix_rank(X1)\n",
    "print('Rank', rank) # Returns: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-corps",
   "metadata": {},
   "source": [
    "In this code, we create the input matrix X with the collinear variables temp and temp_C. We then add a column of ones and end up with an input matrix X1 with three columns. **This matrix is rank-deficient because its number of independent features is 2. This number is called the rank of the matrix, and we say that X1 is rank-deficient because its rank is not maximal, i.e., it's not 3.**\n",
    "\n",
    "### Collinearity in practice\n",
    "\n",
    "**In theory, the OLS solution doesn't exist when \n",
    "X\n",
    " contains collinear features. However, most machine learning tools can handle** these situations and return a vector of parameters \n",
    "⃗\n",
    "w\n",
    " as if there were no collinear features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "listed-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [155.34445517  27.10638524  31.24446504]\n",
      "rank: 2\n",
      "RSS: []\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import lstsq\n",
    "\n",
    "# Compute OLS using lstsq\n",
    "w, rss, rank, _ = lstsq(X1, users)\n",
    "\n",
    "print('w:', w) \n",
    "print('rank:', rank) \n",
    "print('RSS:', rss) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-adventure",
   "metadata": {},
   "source": [
    "In this code, we call the **lstsq() function with the rank-deficient matrix X1 which returns a set of parameters w,** the rank of the matrix, and the **RSS score which is an array of length zero when the matrix is rank-deficient.** The function also returns the singular values of the matrix (the fourth return value), but we can discard this result by assigning it to the \"throwaway\" variable _.\n",
    "\n",
    "Let's compare the performance of this model to a simple linear regression with the \n",
    "R\n",
    "2\n",
    " coefficient. We can use the r2_score(y,y_pred) function from the Scikit-learn metrics module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structural-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 normal: 0.5954233080185317\n",
      "R^2 collinear: 0.5954233080185317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# R^2 coefficient of simple linear regression\n",
    "coefs = np.polyfit(temp, users, deg=1)\n",
    "y_pred_normal = np.polyval(coefs, temp)\n",
    "r2_normal = r2_score(users, y_pred_normal)\n",
    "print('R^2 normal:', r2_normal)\n",
    "\n",
    "# R^2 coefficient with collinear features\n",
    "y_pred_collinear = np.matmul(X1, w)\n",
    "r2_collinear = r2_score(users, y_pred_collinear)\n",
    "print('R^2 collinear:', r2_collinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-diagnosis",
   "metadata": {},
   "source": [
    "**We can see that collinearity didn't affect performance in this case.**\n",
    "\n",
    "### Nearly collinear features\n",
    "ometimes, features are highly correlated but there isn't a perfect linear relationship between them. These are nearly collinear features.\n",
    "\n",
    "For instance, say that we measure temperatures with two different thermometers. One gives temperatures in degrees Celsius and the other in degrees Fahrenheit. To simulate this scenario, we can simply convert the temp_C values to degrees Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "employed-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 3\n",
      "RMSE: 233.28290574613763\n",
      "w: [19735.20830565  1133.01833753  -611.75287897]\n"
     ]
    }
   ],
   "source": [
    "# Convert to degrees Fahrenheit\n",
    "temp_F = 1.8*temp_C + 32\n",
    "\n",
    "# Add small variations\n",
    "noise = np.random.normal(loc=0, scale=0.01, size=temp_F.shape)\n",
    "temp_F += noise\n",
    "\n",
    "# Create input matrix X\n",
    "X = np.c_[temp_C, temp_F]\n",
    "\n",
    "# Compute OLS using lstsq\n",
    "X1 = np.c_[np.ones(X.shape[0]), X] # Create X1 matrix\n",
    "w, rss, rank, _ = lstsq(X1, users) # OLS\n",
    "\n",
    "print('rank:', rank) # Returns: 3\n",
    "print('RMSE:', np.sqrt(rss/len(users))) # Depends on the noise value\n",
    "print('w:', w) # Depends on the noise value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-christmas",
   "metadata": {},
   "source": [
    "**This time, X1 is full rank and the lstsq() function returns the residual sum of squares. If you run the code several times, you can see that the coefficients vary a lot.**\n",
    "\n",
    "### This is due to ill-conditioning.\n",
    "### Ill-conditioning\n",
    "\n",
    "In the example from above, **X1 has full-rank and the OLS solution exists**. However, it's numerically unstable. A small change in the data produces very different coefficients. We can quantify this phenomenon with the **condition number**. Inverting a matrix with a large condition number is **numerically unstable**.\n",
    "\n",
    "We can compute the condition number of X1 with the cond() function from the Numpy linalg module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "certain-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number: 207762.50372569985\n"
     ]
    }
   ],
   "source": [
    "# Condition number\n",
    "cn = np.linalg.cond(X1)\n",
    "print('Condition number:', cn) # Depends on the noise value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-table",
   "metadata": {},
   "source": [
    "The value of cn depends on the noise in the X1 matrix from above. However, you should get a value above 200 thousand. **By increasing the scale of the noise, you decrease the correlation between the two variables and you should see that it reduces the condition number.**\n",
    "\n",
    "Again, ill-conditioning doesn't necessarily affect the predictive accuracy of the model. In most cases, it will simply result in large variations in the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "smart-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 nearly collinear: 0.5957144888659831\n"
     ]
    }
   ],
   "source": [
    "# Same with the nearly collinear matrix\n",
    "y_pred_nearcol = np.matmul(X1, w)\n",
    "r2_nearcol = r2_score(users, y_pred_nearcol)\n",
    "\n",
    "# R^2 coefficient with nearly collinear features\n",
    "print('R^2 nearly collinear:', r2_nearcol)\n",
    "# should be around 0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-encounter",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "One way to solve ill-conditioning is to create a **constraint on the coefficients**. The idea is to modify the **objective function and add a regularization term that penalizes** large coefficients.\n",
    "\n",
    "Scikit-learn implements regularization with the Ridge estimator which is similar to the LinearRegression one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "employed-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 7.46305562 13.51599612]\n",
      "Intercept: -272.0801819956971\n",
      "R^2: 0.5954284128001864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Add small variations\n",
    "noise = np.random.normal(loc=0, scale=0.01, size=temp_F.shape)\n",
    "temp_F = (1.8*temp_C + 32) + noise\n",
    "\n",
    "# Create input matrix X\n",
    "X = np.c_[temp_C, temp_F]\n",
    "\n",
    "# Fit a Ridge regression\n",
    "ridge = Ridge(alpha=100)\n",
    "ridge.fit(X, users)\n",
    "\n",
    "print('Coefficients:', ridge.coef_)\n",
    "print('Intercept:', ridge.intercept_)\n",
    "print('R^2:', ridge.score(X, users))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-seven",
   "metadata": {},
   "source": [
    "In this code, we create an input matrix X of nearly collinear features, and we fit the Ridge model to it. Note that we pass the X matrix to the fit() function (i.e., without the column of ones) since Scikit-learn objects automatically compute the intercept term. Also, we use an alpha parameter which controls the regularization strength. Finally, we print the model coefficients and the \n",
    "R\n",
    "2\n",
    " score.\n",
    "\n",
    "If you run the code several times, you should see that the coefficient are more stable than before.\n",
    "\n",
    "It's interesting to note that the second run has an \n",
    "R\n",
    "2\n",
    " score of 0.595460937325 which is slightly better than the one of the simple linear regression model from above 0.595423308019. This is due to **overfitting - the model fits the noise in the temp_F variable.**\n",
    "\n",
    "But we will learn more about regularization, overfitting and the Ridge estimator later in this course.\n",
    "\n",
    "### Summary\n",
    "Let's summarize what we've learned in this unit. Here are a few takeaways.\n",
    "\n",
    "- Collinearity happens when there is an **exact linear relationship** between one or more features. In this case, the \n",
    "X\n",
    " matrix, with the additional column of ones, and its moment matrix \n",
    "X\n",
    "⊺\n",
    "X\n",
    " **are rank-deficient and the OLS solution doesn't exist.**\n",
    "- Nearly collinear features make **computations numerically unstable** which result in large variations in the model coefficients. This is called **ill-conditioning**. It's numerically unstable to compute the inverse of matrices with a **large condition number**.\n",
    "- Regularization stabilizes model coefficients.\n",
    "\n",
    "Ill-conditioning is a complex topic. You can take a look at this thread (https://stats.stackexchange.com/questions/168622/why-is-multicollinearity-not-checked-in-modern-statistics-machine-learning) if you want to learn more about it. However, note that some answers refer to models that we will see later in this course.\n",
    "\n",
    "In the next exercise, you will fit a linear regression model to a data set with multiple features (including collinear and nearly collinear ones). This is an excellent opportunity to experiment with ill-conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-prisoner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
