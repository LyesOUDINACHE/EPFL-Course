{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67431727",
   "metadata": {},
   "source": [
    "# Keras API\n",
    "\n",
    "So far, we worked directly on the **computational graphs with the low-level APIs.** TensorFlow relies on those graphs internally and it's a good idea to always think in terms of nodes, operations, graphs, and sessions. However, we don't always need to work with those concepts directly and can leverage higher-level APIs that simplify the design and training process.\n",
    "\n",
    "In this unit, we present **the Keras API which is now the recommended TensorFlow official high-level interface.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713543c",
   "metadata": {},
   "source": [
    "### Keras model\n",
    "\n",
    "Keras functionalities can be found under the tf.keras module. In this unit, we will mainly work with the objects from its layers submodule and assemble them in a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c85c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lyeso\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dda89d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 12,730\n",
      "Trainable params: 12,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=28*28))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884512d",
   "metadata": {},
   "source": [
    "In this code, we first create the Sequential model and add each layer object. In our case, we create Dense layers by specifying the number of units, the activation function and the input dimensions i.e. a flat vector with 28x28=784 values. Finally, we print a summary of our network that displays the different layers and their number of parameters. As we can see, most of them are in the hidden layer with 12,560 parameters i.e. 784x16=12,544 weights and 16 bias values.\n",
    "\n",
    "> **Note**: When we use TensorFlow's low-level interface, we set the last layer activation to None and then use the softmax_cross_entropy_with_logits loss function which computes the class probability distribution (softmax) and the loss value (cross-entropy) in one operation to avoid potential numerical issues. You can read more about this in this article https://blog.feedly.com/tricks-of-the-trade-logsumexp/. Keras handles this internally, and we don't have to combine them into one operation: we can set the output layer activation to softmax and then use the cross-entropy loss function when defining the optimizer. One important difference is that our network will output the class probability distribution directly instead of the class scores (logits).\n",
    "\n",
    "By default, **Keras initializes the weights with the variance scaling technique that we have seen in the last subject.** However, we can also manually specify the initializers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44320a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 12,730\n",
      "Trainable params: 12,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden layer\n",
    "model.add(Dense(\n",
    "    units=16, activation=activations.relu, input_dim=28*28,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=2.0, seed=0)))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(\n",
    "    units=10, activation=activations.softmax,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=0)))\n",
    "\n",
    "# Print network summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26d211",
   "metadata": {},
   "source": [
    "This network is equivalent to the one from above but fixes the seed number to get reproducible results.\n",
    "\n",
    "As you can see, we didn't specify a graph. Keras automatically adds the network nodes, variables, placeholders, and operations to the default graph and connects them together according to our Sequential model definition.\n",
    "\n",
    "### Loss function and training operation\n",
    "Let's now see how to define the training elements with the Keras syntax:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357749d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Define loss function, optimizer and metrics to track during training\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ab87c",
   "metadata": {},
   "source": [
    "Keras models provide a **compile() function** where we can specify the **optimizer, the loss function and the metrics to monitor.**\n",
    "\n",
    "In the code from above, we used predefined tokens such as sgd or acc which are good defaults values. Again, we can also have more control over the different parameters by manually defining the objects from the Keras modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6e3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "# Define loss function, optimizer, and metrics to track during training\n",
    "model.compile(\n",
    "    optimizer='sgd', # .. or optimizers.SGD(lr=0.01)\n",
    "    loss='sparse_categorical_crossentropy', # .. or losses.sparse_categorical_crossentropy\n",
    "    metrics=['acc'] # .. or metrics.sparse_categorical_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b460b0",
   "metadata": {},
   "source": [
    "This code is equivalent to the one from above, but we can now specify the learning rate and the other parameters of the optimizer.\n",
    "\n",
    "### Load/split data\n",
    "\n",
    "Before training our MNIST classifier, let's load and split the data. Keras already provides many data sets from its datasets module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9ed95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 28, 28) (60000,)\n",
      "Test: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509fe7e",
   "metadata": {},
   "source": [
    "In this code, we use the load_data() function from its mnist module which returns 60k-10k train/test splits with the 28x28 grayscale images and the 0-9 numerical labels.\n",
    "\n",
    "Our dense network works with flat input vectors. So let's reshape and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799b5cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 784)\n",
      "Test: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Rescale train and validation data\n",
    "scaler = StandardScaler()\n",
    "X_train_preprocessed = scaler.fit_transform(X_train.reshape([-1, 784]).astype(float))\n",
    "X_test_preprocessed = scaler.transform(X_test.reshape([-1, 784]).astype(float))\n",
    "\n",
    "print('Train:', X_train_preprocessed.shape) #\n",
    "print('Test:', X_test_preprocessed.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3956a",
   "metadata": {},
   "source": [
    "### Training\n",
    "So far, we manually implemented our own training loops from tf.Session objects. Keras encapsulates this process in its Model fit() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990d4678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(32, 16), b.shape=(16, 10), m=32, n=10, k=16\n\t [[{{node dense_3/MatMul}} = MatMul[T=DT_FLOAT, _class=[\"loc:@training/SGD/gradients/dense_3/MatMul_grad/MatMul\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_2/Relu, dense_3/MatMul/ReadVariableOp)]]\n\t [[{{node loss_1/dense_3_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_1/_61}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_113_l...t/Switch_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-977e08b05286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;31m# Shuffle training samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\exts-ml\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(32, 16), b.shape=(16, 10), m=32, n=10, k=16\n\t [[{{node dense_3/MatMul}} = MatMul[T=DT_FLOAT, _class=[\"loc:@training/SGD/gradients/dense_3/MatMul_grad/MatMul\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_2/Relu, dense_3/MatMul/ReadVariableOp)]]\n\t [[{{node loss_1/dense_3_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_1/_61}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_113_l...t/Switch_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history = model.fit(\n",
    "    x=X_train_preprocessed, y=y_train,\n",
    "    validation_split=0.2, batch_size=32, epochs=50,\n",
    "    shuffle=True # Shuffle training samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a34361",
   "metadata": {},
   "source": [
    "Keras prints the training/validation loss and accuracy metrics that we specified in the metrics=['acc'] argument from the compile() step.\n",
    "\n",
    "We can access those values via the history object returned by fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The history object saved the scores during training\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece01669",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create two plots: one for the loss value, one for the accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy values\n",
    "ax1.plot(history.history['loss'], label='train loss')\n",
    "ax1.plot(history.history['val_loss'], label='val loss')\n",
    "ax1.set_title('Validation loss {:.3f} (mean last 3)'.format(\n",
    "    np.mean(history.history['val_loss'][-3:]) # last three values\n",
    "))\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss value')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracy values\n",
    "ax2.plot(history.history['acc'], label='train acc')\n",
    "ax2.plot(history.history['val_acc'], label='val acc')\n",
    "ax2.set_title('Validation accuracy {:.3f} (mean last 3)'.format(\n",
    "    np.mean(history.history['val_acc'][-3:]) # last three values\n",
    "))\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778efd4",
   "metadata": {},
   "source": [
    "We can see that the **validation loss and accuracy reach a plateau after 10 epochs. The model then starts overfitting.**\n",
    "\n",
    "It's interesting to note that we didn't specify any session. Again, Keras automatically handles this object internally for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c954631",
   "metadata": {},
   "source": [
    "### Evaluate test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_loss, test_accuracy) = model.evaluate(X_test_preprocessed, y_test, batch_size=32)\n",
    "\n",
    "print('Test loss: {:.2f}'.format(test_loss))\n",
    "print('Test accuracy: {:.2f}%'.format(100*test_accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df24099",
   "metadata": {},
   "source": [
    "In this code, we use the Model evaluate() function to evaluate the metrics on a new data set. Note that we can specify a batch size such that the evaluation is done by batches of data examples to reduce memory usage.\n",
    "\n",
    "The function returns the metrics in the same order as the metrics_names model attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518305c4",
   "metadata": {},
   "source": [
    "### Visualize weights\n",
    "We can extract the layers from our Sequential model with the layers attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45915b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve first hidden layer\n",
    "hidden = model.layers[0]\n",
    "\n",
    "# Get weights/biases\n",
    "weights_hidden, biases_hidden = hidden.get_weights()\n",
    "\n",
    "# Create figure with 16 subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(5, 5))\n",
    "\n",
    "# Plot the weights of the 16 hidden units\n",
    "for i, axis in enumerate(axes.flatten()):\n",
    "    # Get weights of i-th hidden unit\n",
    "    weights = weights_hidden[:, i]\n",
    "\n",
    "    # Reshape into 28 by 28 array\n",
    "    weights = weights.reshape(28, 28)\n",
    "\n",
    "    # Plot weights\n",
    "    axis.set_title('unit {}'.format(i+1), size=9, pad=2)\n",
    "    axis.imshow(weights, cmap=plt.cm.gray_r) # Grayscale\n",
    "    axis.get_xaxis().set_visible(False) # Disable x-axis\n",
    "    axis.get_yaxis().set_visible(False) # Disable y-axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081a58e",
   "metadata": {},
   "source": [
    "The weights look a bit noisy, adding regularization could potentially help\n",
    "\n",
    "> **Challenge** - Try with dropout by adding tf.keras.layers.Dropout(x) layers to the Sequential model. Also try with regularization by setting the kernel_regularizer=tf.keras.regularizers.l2(x) parameters from Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33754bf8",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Create model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Hidden layer\n",
    "model2.add(Dense(\n",
    "    units=16, activation=activations.relu, input_dim=28*28,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=2.0, seed=0)))\n",
    "\n",
    "model2.add(Dropout(rate=0.2))\n",
    "\n",
    "# Output layer\n",
    "model2.add(Dense(\n",
    "    units=10, activation=activations.softmax,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=0)))\n",
    "\n",
    "\n",
    "model2.compile(\n",
    "    optimizer='sgd', # .. or optimizers.SGD(lr=0.01)\n",
    "    loss='sparse_categorical_crossentropy', # .. or losses.sparse_categorical_crossentropy\n",
    "    metrics=['acc'] # .. or metrics.sparse_categorical_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "history2 = model2.fit(\n",
    "    x=X_train_preprocessed, y=y_train,\n",
    "    validation_split=0.2, batch_size=32, epochs=50,\n",
    "    shuffle=True # Shuffle training samples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_loss2, test_accuracy2) = model2.evaluate(X_test_preprocessed, y_test, batch_size=32)\n",
    "\n",
    "print('Test loss: {:.2f}'.format(test_loss2))\n",
    "print('Test accuracy: {:.2f}%'.format(100*test_accuracy2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea946c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two plots: one for the loss value, one for the accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy values\n",
    "ax1.plot(history2.history['loss'], label='train loss')\n",
    "ax1.plot(history2.history['val_loss'], label='val loss')\n",
    "ax1.set_title('Validation loss {:.3f} (mean last 3)'.format(\n",
    "    np.mean(history2.history['val_loss'][-3:]) # last three values\n",
    "))\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss value')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracy values\n",
    "ax2.plot(history2.history['acc'], label='train acc')\n",
    "ax2.plot(history2.history['val_acc'], label='val acc')\n",
    "ax2.set_title('Validation accuracy {:.3f} (mean last 3)'.format(\n",
    "    np.mean(history2.history['val_acc'][-3:]) # last three values\n",
    "))\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3899302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve first hidden layer\n",
    "hidden2 = model2.layers[0]\n",
    "\n",
    "# Get weights/biases\n",
    "weights_hidden2, biases_hidden2 = hidden2.get_weights()\n",
    "\n",
    "# Create figure with 16 subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(5, 5))\n",
    "\n",
    "# Plot the weights of the 16 hidden units\n",
    "for i, axis in enumerate(axes.flatten()):\n",
    "    # Get weights of i-th hidden unit\n",
    "    weights2 = weights_hidden2[:, i]\n",
    "\n",
    "    # Reshape into 28 by 28 array\n",
    "    weights2 = weights2.reshape(28, 28)\n",
    "\n",
    "    # Plot weights\n",
    "    axis.set_title('unit {}'.format(i+1), size=9, pad=2)\n",
    "    axis.imshow(weights2, cmap=plt.cm.gray_r) # Grayscale\n",
    "    axis.get_xaxis().set_visible(False) # Disable x-axis\n",
    "    axis.get_yaxis().set_visible(False) # Disable y-axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934cebc",
   "metadata": {},
   "source": [
    "### Regularization L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create model\n",
    "model3 = Sequential()\n",
    "\n",
    "# Hidden layer\n",
    "model3.add(Dense(\n",
    "    units=16, activation=activations.relu, input_dim=28*28,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=2.0, seed=0)))\n",
    "\n",
    "# Output layer\n",
    "model3.add(Dense(\n",
    "    units=10, activation=activations.softmax,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=0),\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(l=0.05))\n",
    ")\n",
    "\n",
    "\n",
    "model3.compile(\n",
    "    optimizer='sgd', # .. or optimizers.SGD(lr=0.01)\n",
    "    loss='sparse_categorical_crossentropy', # .. or losses.sparse_categorical_crossentropy\n",
    "    metrics=['acc'] # .. or metrics.sparse_categorical_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc83515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "history3 = model3.fit(\n",
    "    x=X_train_preprocessed, y=y_train,\n",
    "    validation_split=0.2, batch_size=32, epochs=50,\n",
    "    shuffle=True # Shuffle training samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_loss3, test_accuracy3) = model3.evaluate(X_test_preprocessed, y_test, batch_size=32)\n",
    "\n",
    "print('Test loss: {:.2f}'.format(test_loss3))\n",
    "print('Test accuracy: {:.2f}%'.format(100*test_accuracy3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc401c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two plots: one for the loss value, one for the accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy values\n",
    "ax1.plot(history3.history['loss'], label='train loss')\n",
    "ax1.plot(history3.history['val_loss'], label='val loss')\n",
    "ax1.set_title('Validation loss {:.3f} (mean last 3)'.format(\n",
    "    np.mean(history3.history['val_loss'][-3:]) # last three values\n",
    "))\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss value')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracy values\n",
    "ax2.plot(history3.history['acc'], label='train acc')\n",
    "ax2.plot(history3.history['val_acc'], label='val acc')\n",
    "ax2.set_title('Validation accuracy {:.3f} (mean last 3)'.format(\n",
    "    np.mean(history3.history['val_acc'][-3:]) # last three values\n",
    "))\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f541df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve first hidden layer\n",
    "hidden3 = model3.layers[0]\n",
    "\n",
    "# Get weights/biases\n",
    "weights_hidden3, biases_hidden3 = hidden3.get_weights()\n",
    "\n",
    "# Create figure with 16 subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(5, 5))\n",
    "\n",
    "# Plot the weights of the 16 hidden units\n",
    "for i, axis in enumerate(axes.flatten()):\n",
    "    # Get weights of i-th hidden unit\n",
    "    weights3 = weights_hidden3[:, i]\n",
    "\n",
    "    # Reshape into 28 by 28 array\n",
    "    weights3 = weights3.reshape(28, 28)\n",
    "\n",
    "    # Plot weights\n",
    "    axis.set_title('unit {}'.format(i+1), size=9, pad=2)\n",
    "    axis.imshow(weights3, cmap=plt.cm.gray_r) # Grayscale\n",
    "    axis.get_xaxis().set_visible(False) # Disable x-axis\n",
    "    axis.get_yaxis().set_visible(False) # Disable y-axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e8a94",
   "metadata": {},
   "source": [
    "# Advanced Keras\n",
    "\n",
    "So far, we always worked with in-memory data sets. In this unit, we will see how to scale up and create **data generators that read images from the disk with Keras image generators** and go through a concrete example with a .png version of the CIFAR-10 data set.\n",
    "\n",
    "### Keras Image Generators\n",
    "\n",
    "The first step is to create an **ImageDataGenerator** which specifies how to process each image. In our case, we will simply apply 0-1 rescaling to all images and **additional data augmentation to the train set to increase the size of the dataset** and add some variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7348f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create image generator\n",
    "train_generator = ImageDataGenerator(\n",
    "    rescale=1/255, horizontal_flip=True, rotation_range=5, validation_split=0.2)\n",
    "test_generator = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71f912",
   "metadata": {},
   "source": [
    "We use the rescale parameter to normalize the images between zero and one, and the horizontal_flip and rotation_range transformations to get slightly different variants of the images each time we sample from the images to fight overfitting and virtually augment the data set to improve training. This is why we only use those parameters for the train_generator and not the test_generator one. Optionally, we can also keep a fraction of the data for validation. In this case, we keep 20% of the data as a validation set. Note that they will also go through the training transformations.\n",
    "\n",
    "The second step is to create an iterator that iterates over the directory of CIFAR-10 .png images. The cifar10 folder can be downloaded from the resources of this unit. To iterate over the directory we can use the flow_from_directory() function from our image generators which takes the path to the directory, the batch size and the target size. This can be useful if we need to resize the images before inserting them into our network. However, in our case, we will keep the 32x32 .png images and set target_size=(32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Train, validation and test sets\n",
    "trainset = train_generator.flow_from_directory(\n",
    "    os.path.join('cifar10', 'train'), batch_size=32, target_size=(32, 32),\n",
    "    shuffle=True, subset='training')\n",
    "validset = train_generator.flow_from_directory(\n",
    "    os.path.join('cifar10', 'train'), batch_size=32, target_size=(32, 32),\n",
    "    shuffle=False, subset='validation')\n",
    "testset = test_generator.flow_from_directory(\n",
    "    os.path.join('cifar10', 'test'), batch_size=32, target_size=(32, 32),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc01b89",
   "metadata": {},
   "source": [
    "We create a trainset and a validset iterator for the images from the train directory and a testset one for the images from the test directory. **Note that we don't need to shuffle the validation and test data (i.e we set shuffle=False)** since shuffling only affects training. Also, we only need to specify the subset argument for the train and validation sets since we used above the optional validation_split parameter from the train_generator.\n",
    "\n",
    "We can always verify which images are assigned to each data set by looking at the filenames attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84125862",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "validset.filenames[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cb1c6",
   "metadata": {},
   "source": [
    "As we can see, for each class the first 100 images (1000 validation images/10 class) were assigned to the validation set and the rest to the training one. We can verify that those two sets don't intersect with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ae2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(trainset.filenames).intersection(set(validset.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e95cf",
   "metadata": {},
   "source": [
    "which returns an empty set for the intersection.\n",
    "\n",
    "The directory iterators can be used like normal Python iterators ex. in a for loop. By calling next(), we get the next batch of images with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322275a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_imgs, batch_labels = trainset.next()\n",
    "\n",
    "print('Batch images:', batch_imgs.shape) # (32, 32, 32, 3)\n",
    "print('Batch labels:', batch_labels.shape) # (32, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a3f33",
   "metadata": {},
   "source": [
    "It's important to note that the iterator returns one-hot encoded labels by default. In our case, we have 10 classes, so batch_label is a 32x10 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fa3c9",
   "metadata": {},
   "source": [
    "We can always retrieve the original class names with the class_indices attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33baeb88",
   "metadata": {},
   "source": [
    "Note that it is possible to obtain integer encoded labels with the iterator. To do that, one can change the default parameter class_mode = 'categorical' in flow_from_directory() function to class_mode = 'sparse'.\n",
    "\n",
    "### Training with generators\n",
    "Let's now see how to train our network with iterators. But first, we need to create the network. This time, we will implement a ConvNet similar to the one we saw in the last subject. To achieve this, we will add Conv2D, MaxPool2D, Flatten and Dense layers to our Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "# Convolutional Network\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=5, strides=2,\n",
    "                              activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,\n",
    "                              activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(units=trainset.num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2347b",
   "metadata": {},
   "source": [
    "Let's compile the model with the Adam optimizer and the categorical_crossentropy loss function which is equivalent to the sparse_categorical_crossentropy one from the last unit, but works with one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a797085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad48ece",
   "metadata": {},
   "source": [
    "Let's also create an optional EarlyStopping callback that we can use during the model fitting process. Callbacks are functions that can be applied at different stages of the training procedures. In our case, EarlyStopping will monitor the loss value on the validation set and stop the training once this loss value didn't improve anymore for more than 6 epochs. Early stopping is a common way to fight overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75818189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End training when accuracy stops improving (optional)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c051da",
   "metadata": {},
   "source": [
    "Now that everything is ready we can implement the model fitting. This time, we need to use the fit_generator() to train the network using data generators. Instead of passing the x/y inputs as we did previously with model.fit(), this function takes a generator which returns the training inputs/labels batches and a validation_data one which returns the validation batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit_generator(\n",
    "    generator=trainset, validation_data=validset, epochs=100, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc786ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
