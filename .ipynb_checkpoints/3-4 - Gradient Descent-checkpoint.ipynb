{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-public",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In the last subject, we learned about the ordinary least squares (OLS) solution which is the most common way to find the set of parameters of a linear regression model that minimizes the squares of the residuals. We will now see another method called gradient descent (GD) which is a generic approach that can find the optimal parameters of a variety of models and cost functions.\n",
    "\n",
    "Just like OLS, gradient descent uses the gradient of the cost function to minimize it. In this unit, we will see the basic idea behind the algorithm. We will start with a recap about derivatives and then see why it's very natural to use them to optimize functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-hormone",
   "metadata": {},
   "source": [
    "Hence, to minimize the value of the function \n",
    "f\n",
    ", we should take a step in the opposite direction of its derivative. Equivalently, we can say that the derivative shows the direction of increase. This observation also applies to functions with multiple parameters. In that case, the gradient \n",
    "âˆ‡\n",
    "f\n",
    " gives us the direction of the steepest increase. This may sound abstract, so let's take an example.\n",
    "\n",
    "This image shows the gradient of the function in blue with two parameters. The red arrows correspond to the values of the gradient at different \n",
    "(\n",
    "x\n",
    ",\n",
    "y\n",
    ")\n",
    " locations. They have a dimension of two because the function has two parameters and hence two partial derivatives.\n",
    " \n",
    " The idea is that this plot could be the error surface of a simple linear regression model where the \n",
    "x\n",
    " and \n",
    "y\n",
    " variables correspond to the \n",
    "a\n",
    ", \n",
    "b\n",
    " parameters and the cost function (an exotic one by the look of it) to \n",
    "f\n",
    "(\n",
    "x\n",
    ",\n",
    "y\n",
    ")\n",
    ".\n",
    "\n",
    "\n",
    "### Local minimum\n",
    "In the gradient descent method, we take small steps in the opposite direction of the gradient. Hence, it might return suboptimal solutions when the function is not convex. A function is not convex if it's possible to find a line, joining two points on the function, which intersects with it. Again, this may sound abstract, so let's take an example.\n",
    "\n",
    "Here are three functions. The left one is convex, and the two others are not. We indicate the minimum value with a green dot.\n",
    "\n",
    "\n",
    "In the convex case, regardless of where we start on the curve, we will always end up on the green point if we follow the direction of descent. On the other hand, there are no guarantees that we find the optimal solution in the two nonconvex cases. If we start in the wrong place, it's possible that we get stuck in a local minimum (the red dots).\n",
    "\n",
    "### Summary\n",
    "Let's summarize what we've learned in this unit. Here are a few takeaways.\n",
    "\n",
    "- Gradient descent follows the opposite direction of the gradient.\n",
    "- It may return suboptimal solutions when the function is not convex.\n",
    "\n",
    "In the next unit, we will see how to implement the algorithm for the simple linear regression model with the mean squared error (MSE) objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-mount",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-alcohol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
