{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cba83d7",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "### Overfitting and Occam's Razor\n",
    "\n",
    "In the last subject, we discussed model complexity and the ability to generalize from data. We saw two cases.\n",
    "\n",
    "- Underfitting - the model is too simple and fails to fit the data/signal\n",
    "- Overfitting - the model is too complex and fits the noise in addition to the signal\n",
    "\n",
    "In this subject, we will see how to control **overfitting using regularization**. But first, **let's talk about Occam's razor** which is the basic idea behind it, but also an interesting principle in general.\n",
    "\n",
    "### Occam's razor\n",
    "Occam's razor is a principle which states that if multiple solutions are available, the simplest one is better than the others. **The idea is that it's easy to build overly complicated solutions with ad-hoc rules that don't generalize well.**\n",
    "\n",
    "In the context of machine learning, the principle says that we should prefer simpler models unless we are sure that the complex ones are necessary.\n",
    "\n",
    "We often say that generalization is the central goal of machine learning. Occam's razor is one of the important principles to achieve this. You can take a look at section 3 and 4 of the paper \"A few useful things to know about machine learning\" by Pedro Domingos to learn more about the intuition behind generalization. Here is the link to the google scholar page. https://scholar.google.ch/scholar?cluster=4404716649035182981&hl=en&as_sdt=0,5\n",
    "\n",
    "### Increasing the amount of data\n",
    "The amount of data also plays a role in the under-/overfitting balance. Let's do a quick experiment. In this image, we show two polynomial regressions of degree 9 fitted to 10 and 80 data points from the same source of data.\n",
    "\n",
    "https://d7whxh71cqykp.cloudfront.net/uploads/image/data/3734/data-size.svg\n",
    "\n",
    "In the first case, the model is strongly overfitting. In fact, the polynomial passes through each data point. The problem is lessened in the second case.\n",
    "\n",
    "### Summary\n",
    "In this unit, we learned about Occam's razor which is an important principle in machine learning. In the next unit, we will learn about regularization which is an efficient way to reduce overfitting.\n",
    "\n",
    "\n",
    "# Regularziation\n",
    "\n",
    "In practice, we use regularization to fight overfitting. In this unit, we will see the basic idea behind it. We will then implement regularization with Scikit-learn in the next unit.\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "hen fitting a model, we are searching for a set of optimal parameters \n",
    "⃗\n",
    "w\n",
    " that minimize the loss function \n",
    "L\n",
    "(\n",
    "⃗\n",
    "w\n",
    ")\n",
    ".\n",
    "\n",
    "As you can see, there are **no constraints on the parameters**\n",
    "⃗\n",
    "w\n",
    ". \n",
    "In particular, **they can get very large as long as they minimize the loss function.** However, **large coefficients is one of the symptoms of overfitting.** **With large coefficients, a small variation in the input data has a big effect on the predictions.**\n",
    "\n",
    "The idea behind **regularization is to add a constraint on the value of the parameters**. In practice, we include a penalization term in the cost function that measures how large the parameters are. For instance, \n",
    "L\n",
    "2\n",
    " regularization measures the squares of the parameters \n",
    "w\n",
    "i\n",
    ".\n",
    "\n",
    "minL(⃗w)+α * ∑w2i\n",
    "\n",
    "When \n",
    "α\n",
    " tends toward zero, the constraint on the parameters vanishes, and the problem is the same as before. When \n",
    "α\n",
    " tends toward infinity, the \n",
    "L\n",
    "(\n",
    "⃗\n",
    "w\n",
    ")\n",
    " term becomes irrelevant compared to the \n",
    "L\n",
    "2\n",
    " one. In this case, all parameters are zero except the intercept term \n",
    "w\n",
    "0\n",
    ".\n",
    "\n",
    "### Geometrical interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f39f",
   "metadata": {},
   "source": [
    "*Note: it's not necessary to understand the mathematics behind this - here, we introduce this geometrical interpretation because it can help us visualize the effect of regularization. However, if you are curious about the maths behind this interpretation and want to learn more about Lagrange multipliers, you can take a look a this excellent tutorial from khanacademy.org.*\n",
    "\n",
    "In practice, we never use this formulation to fit our models. However, it's useful because it provides a nice geometrical interpretation - For a model with two parameters \n",
    "w\n",
    "1\n",
    " and \n",
    "w\n",
    "2\n",
    ", we are searching for a point inside a circle of radius \n",
    "c\n",
    ". In three dimensions, we are searching for a set of values inside a sphere of radius \n",
    "c\n",
    ". Here is an illustration of the two-dimensional case.\n",
    "\n",
    "https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2938/regularization-geometrical-interpretation.svg\n",
    "\n",
    "In this image, the blue point outside the circle represents the parameters \n",
    "w\n",
    "1\n",
    " and \n",
    "w\n",
    "2\n",
    " that minimize the unconstrained loss value \n",
    "L\n",
    "(\n",
    "⃗\n",
    "w\n",
    ")\n",
    ". This minimal value is not inside the circle of radius \n",
    "c\n",
    ". Hence it's not a valid solution according to the constraint. The solution that minimizes the cost function inside the gray circle is denoted \n",
    "w\n",
    "∗\n",
    " (\n",
    "w\n",
    " star).\n",
    " \n",
    " ### Other regularizers\n",
    " \n",
    " We will use \n",
    "L\n",
    "2\n",
    " regularization in this course, but there are other regularizers. For instance, the **lasso regularization \n",
    "L\n",
    "1\n",
    " is a variant of \n",
    "L\n",
    "2**\n",
    " which penalizes the absolute value of the coefficients instead of their squares.\n",
    " \n",
    " https://d7whxh71cqykp.cloudfront.net/uploads/image/data/2937/l1-regularization.svg\n",
    " \n",
    " In other words, with \n",
    "L\n",
    "1\n",
    " regularization, the optimal solution only has a few non-zero parameters, and we say that the solution is sparse which is a desired property in some cases. You can take a look at this thread if you want to learn more about this topic.\n",
    " https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when/answer/Xavier-Amatriain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117395fa",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Let's summarize what we've learned in this unit. Here are a few takeaways.\n",
    "\n",
    "- The idea behind regularization is to add a **constraint** on the amplitude of the coefficients.\n",
    "- This constraint corresponds to an additional term in the cost function called the **penalization term.**\n",
    "- We use an alpha \n",
    "α\n",
    " parameter to control the **regularization strength.**\n",
    "\n",
    "In the next unit, we will implement \n",
    "L\n",
    "2\n",
    " regularization for linear regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435391d1",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    " the last unit, we learned about \n",
    "L\n",
    "2\n",
    " regularization and saw that it adds a constraint on the length of the vector of parameters \n",
    "⃗\n",
    "w\n",
    ". So far, **we didn't specify any particular model or cost function**, but if we use **multi-linear regressions and minimize the squares of the residuals**, we obtain the ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf706b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
